{"md_content":"# Classification of Melanoma and Nevus in Digital Images for Diagnosis of Skin Cancer  \n\nMUHAMMAD QASIM KHAN 1, AYYAZ HUSSAIN1, SAEED UR REHMAN 2, UMAIR KHAN3, MUAZZAM MAQSOOD 3, KASHIF MEHMOOD3, AND MUAZZAM A. KHAN4  \n\n1Department of Computer Science and Software Engineering, International Islamic University, Islamabad 44000, Pakistan   \n2Department of Electrical and Computer Engineering, COMSATS University Islamabad, Attock Campus, Attock 43600, Pakistan   \n3Department of Computer Science, COMSATS University Islamabad, Attock Campus, Attock 43600, Pakistan   \n4Department of Computing, SEECS, National University of Sciences and Technology, Islamabad 44000, Pakistan  \n\nCorresponding author: Muazzam Maqsood (muazzam.maqsood@cuiatk.edu.pk)  \n\nABSTRACT Melanoma is considered a fatal type of skin cancer. However, it is sometimes hard to distinguish it from nevus due to their identical visual appearance and symptoms. The mortality rate because of this disease is higher than all other skin-related consolidated malignancies. The number of cases is growing among young people, but if it is diagnosed at an earlier stage, then the survival rates become very high. The cost and time required for the doctors to diagnose all patients for melanoma are very high. In this paper, we propose an intelligent system to detect and distinguish melanoma from nevus by using the stateof-the-art image processing techniques. At first, the Gaussian filter is used for removing noise from the skin lesion of the acquired images followed by the use of improved K-mean clustering to segment out the lesion. A distinctive hybrid superfeature vector is formed by the extraction of textural and color features from the lesion. Support vector machine (SVM) is utilized for the classification of skin cancer into melanoma and nevus. Our aim is to test the effectiveness of the proposed segmentation technique, extract the most suitable features, and compare the classification results with the other techniques present in the literature. The proposed methodology is tested on the DERMIS dataset having a total number of 397 skin cancer images: 146 are melanoma and 251 are nevus skin lesions. Our proposed methodology archives encouraging results having $96\\%$ accuracy.  \n\nINDEX TERMS Melanoma, nevus, feature, K-means clustering, and centroid selection.  \n\n# I. INTRODUCTION  \n\nSkin cancer is measured as a major contributor to the causes of deaths around the world [1]. There are various types of cancers that are discovered and battled with. However, skin cancer is amongst fast-growing cancer nowadays. According to modern research, patients with a skin cancer diagnosis is significantly increasing more than any other cancer form every year [2]. Melanoma is the most common form of skin disease that affects the skin surface cells known as melanocytes. It consists of cells that cause the skin to turn to black color [3]. Melanoma can be found in dark or darker color yet at some point it might likewise be in the skin, pink, red, purple, blue or white color [4]. This form of cancer is very disturbing due to its tendency to cause metastasis, i.e. ability to spread. Melanoma can be found anywhere on the human body, however, it is mostly developed on the back of human legs [5].  \n\nDetecting skin cancer at the initial stage can help in reducing the risk factor in patients. Different skin cancer types can be found in figure 1.  \n\nAccording to the research, the mortality rate may be reduced up to $90\\%$ , if the skin cancer is diagnosed at an initial stage, Hence the diagnosis and classification of the skin cancer in its early stage are vitally important [4], [5]. Among the conventional approaches followed by researchers to detect melanoma and nevus is ABCD rule [5]. A total dermoscopic score is obtained for each of the ABCD features where $A$ represents Asymmetry, $B$ is for Border irregularity, $C$ represents color variations and $D$ is for the Diameter. Each respective feature is assigned an individual weight based on their significance in the feature space. Based on the calculated score the lesion is identified as cancerous or benign. A 7-point Checklist is another technique that is used to identify skin cancer in dermoscopic images. The list target symptoms of atypical pigment network, grey-blue areas and atypical vascular pattern, streaks, blotches, irregular dots and globules, and regression patterns. At times when these symptoms are identified, a medical professional is consulted for the treatment [8]. Later on, the checklist reduced to a lesser number of features of the different network, Asymmetry and blue-white structures[9]. Considering the complex nature of melanoma, it becomes hard for the researchers to detect skin cancer only on the basis of these geometrical features. Another problem is that the size of the image database is increasing dramatically. So the practicality of such information is dependent on how well it can be accessed, searched and how well the relevant knowledge can be extracted from it.  \n\n![](images/7cc46b0da2e112c93375b2b982b248fd426c8aee4f96435511d9cb279618e7d2.jpg)  \nFIGURE 1. Types of skin cancer.  \n\nWith the advent of computer-aided diagnostic systems, researcher mainly emphasizes on the automatic detection and classification of skin cancer. Medical images in the form of textural features [10]–[12], geometric features [4], [13]–[15], color features and in a combination [16]–[18] have been used to identify and classify skin cancer diseases. However, it is still a challenging task to identify most discriminative features for identifying melanoma at its initial stage [19].  \n\nOur research work aims to achieve high accuracy results in identifying and classifying skin cancer, contributing to the present literature by,  \n\n• To develop a complete automated computer-aided system to detect melanoma cancer accurately.   \n• Design of an improved K-Mean approach for computationally efficient segmentation.   \n• Utilization of hybrid features incorporating both texture and color of the lesion.  \n\nThe remaining of our research paper is organized as a detail literature review of existing techniques of features extracting and classification is discussed in Section 2. The components of the proposed work are discussed in detail in section 3. Section 4 describes the experimental setup and evaluation metrics. Section 5 portrays the results and discussion on the given datasets. The conclusion is given in the last section.  \n\n# II. RELATED LITERATURE  \n\nExtensive research has been done in the respective domain by the discovery and development of new approaches to accurately diagnose skin cancer. Related work can be divided into three types based on features extraction technique i.e. geometrical, textural, and color. These techniques are individually discussed in detail below.  \n\n# A. GEOMETRICAL FEATURES  \n\nThe ABCD-E system [13], [15], [20], 7-point checklist [21], [22], 3-point checklist [23] offers geometrical features techniques that are used for classification of the melanoma.  \n\nAccording to Johr [14], the features extracted by the ABCD rule is computationally less expensive with respect to the 7-point checklist. It is also observed that the consistency in the clinical diagnosis is at a higher rate for the ABCD rule. So, most of the computer-aided systems for melanoma detection used ABCD rule for features extraction. Though, ABCD technique is more prone to over classification of nevi as melanomas [24]. In research work Kasmi and Mokrani [4], extracted the characteristics of ABCD attributes and combines with color asymmetry and dermoscopic structures. The proposed system achieved $91.25\\%$ sensitivity. In another study [25] the author tried to investigate the possibility to automatically detect the dermoscopic patterns based on ABCD rule using deep convolutional neural networks. Experimental results demonstrated $88\\%$ accurate classification of skin cancer.  \n\nIn another research study, Moussa et al. [26] used ABD rule excluding the color from the traditional ABCD, because color requires an additional large amount of computer resources. They utilized KNN as classifier achieving $89\\%$ accuracy.  \n\nIt is also observed that ABCD rubric is sometimes very qualitative and subject manner that results in large inter-observer and intra-observer bias [27]. For this purpose, High-quality intuitive features (HLIF) approach is used that represents the asymmetry characteristic of ABCD rubric from skin cancer images [28]. The system achieved $86\\%$ accuracy from the extracts set of features. Later Amelard et al. [11] extended his previous work by proposing six new HLIF features for different color channels. Experimental results show a better classification accuracy of $94\\%$ .  \n\n# B. COLOR FEATURES  \n\nThe color features can be extracted on the basis of the statistical value calculated from color channels. They are Mean color, variance color and standard deviation value of the RGB or HSI color model. Some of the different color features extraction techniques consist of color asymmetry, centroid distance and LUV histogram distance [29], [30]. In another study [31], the author classified the melanoma on the basis of global and local descriptors. They combined the textural features with the color obtained classification scores of SE and SP are $93\\%$ , $95\\%$ respectively. Other researchers used the same approach using color features with a set of textural and shape-based features. Ganster et al. [6] used color and shape based features from skin lesion with KNN classifier. The number of images taken for the experimentation purpose is more than 5300 and they achieved $87\\%$ and $92\\%$ sensitivity and specificity respectively. Rubegni et al. [32] also utilized textural and geometrical features and achieved a sensitivity of $96\\%$ and specificity of $96\\%$ . Celebi et al. [29] used a large features vector consisted of color, shape and texture features. The SVM achieved a sensitivity of $93\\%$ and specificity of $92\\%$ . Almansour et. al in [16] used color moments with textural features with SVM as classifier achieved $90\\%$ accuracy with 227 melanoma and nonmelanoma skin cancer images.  \n\n# C. TEXTURAL FEATURES  \n\nImage texture represents the spatial distribution of pixel intensity levels in an image. Textural features represent the underlying pattern and layout of intensity levels acting as one of the most distinctive features for object or region of interest identification. When it comes to skin cancer, textural  \n\nfeatures are frequently used for image analysis as it helps in classifying between nevus and melanoma by calculating the irregularity of their structure [33]. It is observed that interest is increasing in the computerized examination of digital images taken by the Dermoscopic process. To improve early identification melanoma classification strategy for the computerized analysis of images obtained from ELM (Epiluminescence microscopy) has been established [6]. Region of interest was extracted using the segmentation algorithm and a combined features strategy was used belonging to shape and radiometric features. KNN classifier was used to achieve $87\\%$ and $92\\%$ sensitivity and specificity respectively.  \n\nThe automatic data analysis for the melanoma early detection system (ADAM) [34] obtained $80\\%$ for both specificity and sensitivity using asymmetry and boundary descriptors with (SVM) as a classifier. Iyatomi et al. [35] used a similar approach and achieved $100\\%$ specificity and $95.9~\\%$ sensitivity. Recent systems in the literature [36] achieved $91\\%$ specificity and $88.2\\%$ sensitivity over a database of 120 skin cancer images. For a proper treatment of the skin cancer computer-aided diagnosis (CAD) techniques helps the physicians to obtain a second opinion about the lesion. Extracting accurate region of interest from skin lesion alongside adjacent portion is required for proper diagnosis and analysis. The authors [37] used active contour and watershed mask for segmentation. They extracted features related to shape, textural and color. The proposed system was tested on 50 images of DERMIS dataset achieving $80\\%$ accuracy. Another novel approach to the Computer Aided Diagnosis (CAD) of initial melanoma has appeared in the process of the web and smartphone applications. In these systems, images were captured with high-resolution cameras instead of using an image database [38]. The system used digital camera images with context knowledge such as type and nature of the skin, sex, age and the body part that is affected. The proposed system also tried to extract the features compatible with the Dermoscopy ABCD rule. The features were further classified in various steps, a processing step for correlation-based feature selection. The system produced specificity of $68\\%$ and a sensitivity of $94\\%$ , on images of 45 Nevus and 107 Melanoma skin cancer images. In another work [39], the author used some non-dermoscopic images of skin cancer where they extracted the ROI or lesion areas using $\\mathbf{k}$ -mean clustering technique and then color and textural features were extracted. Further, a set of visual features were identified by inspecting the dermatologist. For automatic prediction the physician attributes and extraction features were used. A majority vote of all the given predictions was used to achieve final classification. The suggested method resulted in higher diagnostic of $81\\%$ and achieved comparable results to the latest methods that are using skin cancer images. In [40], the melanoma lesion is depicted through a component vector containing texture data, shape, color and also global and local parameters. Some other automated Melanoma detection and classification systems discussed in [41], [42] for the purpose to improve the recognition and classification of the skin lesion. In [43], an approach was presented for feature selection criterion which was built on the arrangement of differential evolution and SVM. Currently, Convolutional Neural Network (CNN) and deep learning-based approaches were used for cancer detection. The computation cost for these approaches became barriers in clinical applications [2], [44], [45]. In the latest research work Dorj et al. [46] classified different skin cancer diseases using deep convolution network. For extracting features a Pre-trained AlexNet convolutional neural network model is used. An ECOC SVM classifier is utilized in the classification of skin cancer. The proposed approach obtained the results of $90\\%$ accuracy for melanoma detection. An enhanced computer automated system was proposed with pixel based technique is used for segmentation [47]. They extracted features using CNN and classify the images through SVM classifier achieving $93\\%$ accuracy on DermIS dataset.  \n\nThe afore-mentioned techniques provide promising results however, they are tested over an insufficient number of images for detailed analysis.  \n\nFrom the literature, it is observed that different features extraction techniques have been used to classify the skin lesion. However, the combination of all these features is still not properly tested. As the skin cancer images can be differentiated based on color variations, therefore, color features should be combined with other features.  \n\n# III. PROPOSED METHODOLOGY  \n\nWe have discussed the components of our proposed methodology in this section. Input images acquired from the datasets undergo quality enhancement through pre-processing techniques. Later the ROI from the skin lesion is extracted which are further processed for significant feature extraction and lastly classifying them into melanoma or nevus. Figure 2 presents the proposed methodology where each stage is briefly discussed in the section below.  \n\n# A. PRE-PROCESSING  \n\nMedical images are often susceptible to noise mainly due to bad illumination, hair and air bubbles [48]. This inclusion of noise in images results in the formation of artifacts. Due to such artifacts, the segmentation results may get affected causing inaccurate detection results. Therefore, noise removal is a significant step before applying any segmentation or feature extraction technique for an accurate diagnosis. To smoothen the image, Gaussian filter is highly recommended as it removes the speckle noise added during the process of acquisition. Gaussian kernel coefficients are sampled from the 2D Gaussian function as shown in equation 1.  \n\n![](images/e5ad2077dea471b2d8200d2845549ea6a2fc4145db4250ca85c5201f1bbd9974.jpg)  \nFIGURE 2. Proposed Methodology for skin Cancer Detection.  \n\n$$\nG(x,y)={\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}}e^{-({\\frac{x^{2}+y^{2})}{2\\sigma^{2}}}}\n$$  \n\nwhere $\\sigma$ is the smoothing factor referring to the amount of blur.  \n\n# B. SEGMENTATION  \n\nK-mean clustering is a common machine learning technique that is extensively used in many applications such as data mining, image processing, and pattern recognition. K-mean is considered as one of the basic methodologies for grouping and clustering of data-points into K number of clusters [18]. It works by splitting the image into non-overlapping groups of pixels based on their intensity levels. The process initiates by selecting the centroids from the data-points either randomly or through a certain criterion. The pixels or data-points are clustered based on their minimum distance from the selected centroids. After each iteration, the mean values of the formed clusters are found and are set as the centroids for the next iteration.  \n\nThe iterative process repeats itself until there is no variation in the successive cluster centroids. The proposed K-Mean initializes its centroids by centroid selection technique as given in equation 2 below.  \n\n![](images/df45d5e9647283d6c8f9737ee7d5b51068738ab9880d1787084b218a84566149.jpg)  \nFIGURE 3. Input image on the left with its equivalent segmented image on the right.  \n\n$$\nm u=(1:K)*\\frac{m}{k+1}\n$$  \n\nwhere $\\mathbf{K}$ is the number of clusters, m is the maximum value of a pixel in the image, and $\\mathbf{k}$ goes from 1 to $\\mathrm{K}$ .  \n\nThe centroid selection technique works by ensuring significant difference among the values of initialized centroids making it more efficient and robust by converging to the final position in a lesser number of iterations.  \n\nIn our proposed system, input images consist of affected lesion surrounded by the background skin. The value of K is taken as 2, such that the foreground lesion is extracted from the background skin as shown in Figure 3.  \n\n# C. FEATURES EXTRACTION  \n\nOnce the lesion is segmented out of the background skin, it is then classified as malignant or benign. For better classification results, it is required to use the best feature descriptors for machine learning modeling. The Increase in the number of features increases the computational cost, inspiring the description of precise decision boundaries. Thus, it is ensured that a distinctive feature set is used.  \n\nA lesion is characterized by its texture and its color. In this research work three different features using Local Binary Pattern (LBP), Grey Level Co-occurrence Matrix (GLCM) and RGB color channel features, are extracted from the ROI of skin lesion. The techniques are utilized to extract the textural and color-based features from the input skin lesion.  \n\n# 1) GREY LEVEL CO-OCCURRENCE MATRIX  \n\nGrey level co-occurrence matrix (GLCM) is a global textural feature extraction technique computing the statistical distribution of intensities in combination at specific positions in the image. Based on the number of contributing intensity levels in the combination, the order of the statistics is determined. GLCM extracts the second order statistical texture features by considering the spatial relationship of two intensity levels.  \n\nA GLCM is a square matrix with dimensions of the number of intensity levels G. The matrix element $\\mathrm{P}(\\mathrm{i},\\mathrm{j})$ specified by the row and column position with i and j, gives the frequency with which the pixel values i and j have occurred overall directions specified by angle $\\theta$ as shown in figure 4.  \n\nAfter the formation of the matrix, various statistical features defined by Haralick et al. [49] are extracted specifying the underlying textural information of the image. Entropy, Angular second moment, variance, etc. are few of the listed features among the Haralick features.  \n\n# 2) LOCAL BINARY PATTERN  \n\nGLCM extracts the global texture of the image giving an overall spatial distribution of image texture however, local texture also needs to be part of the feature vector. Ojala presented a local texture feature extraction technique for an invariant twodimensional Grey scale analysis; Local Binary Pattern [50]. It is used by several applications and is an efficient way and simple operator to denote local patterns. The pixels are tagged by LBP to identify the eight neighborhood pixels with respect to the center value of the window image. The pixels are assigned a binary number based on the threshold value. By comparing the central pixel of the image window with respect to its neighbors LBP code can be calculated as:  \n\n$$\nL B P_{(P,R)}=\\sum_{p=0}^{p-1}S\\left(g_{p}-g_{c}\\right)^{2^{p}},\n$$  \n\nwhere ${\\textbf{S}}(\\mathbf{z})$ is thresholding function, $\\mathrm{g_{c}}$ and $\\mathrm{g}_{\\mathrm{p}}$ denote the Grey level value of the central pixel and it’s neighbor’s value respectively. Here $\\mathrm{\\bfP}$ denotes the overall number of neighbors whereas the value $\\mathbf{R}$ shows the neighborhood’s radius. The LBP algorithm comprises of its roots in 2D texture analysis.  \n\n![](images/9a07ffd7da4cfe373d01af95173772cec9ab9ecd20371606dd8ad448a62e87ad.jpg)  \nFIGURE 4. Formation of a GLCM matrix from the input image (shown as a matrix of intensity levels).  \n\n![](images/e841d001332f578f7ffdeb756adab70144ed54e36be190e4c100ca2c9598af33.jpg)  \nFIGURE 5. LBP represented by binary output matrix calculated from the input image.  \n\nSummarization of the local structure of the image is done by comparing each pixel with its neighbor’s pixels. First of all, take a pixel as center and threshold its neighbors against that pixel. Mark 1 if the center pixel intensity is greater or equal its neighbor and mark 0 if not. We will have a binary number for every pixel, much the same as 10010011. With 8 encompassing pixels, we will finish up with 256 possible combinations, which are named as Local Binary Patterns or LBP codes. The method to compute an LBP code is shown in Figure 5  \n\nThe label image histogram is used as a texture feature following the labeling of pixels with their corresponding LBP codes.  \n\n# 3) COLOR FEATURES  \n\nWhen an object is exposed to a certain wavelength of light it reflects a corresponding color, forming its appearance. By examining the three main colors (red, green, blue), this phenomenon of color space can be better understood [51]. In the segmented lesion, the color feature is used to identify its visible color, this is achieved by utilizing four statistical values such as mean, variation, standard deviation, and skewness through color spaces of RGB. These values are also referred to as color moments or features. Consider C as the color channel with image i, N represents a total number of pixels in a color segmented image, $\\mathbf{k}$ is the kth pixel of that color channel P of an image i with $\\mathbf{N}$ pixels in a color space. Hence the color features can be defined as below. Here mean shows the average value of each color in RGB color space in equation 4.  \n\n$$\n\\mu=\\frac{1}{N}\\sum_{k=1}^{N}C\n$$  \n\n![](images/236729099058f2814c1af5c98f5723fafb7e13d2aa1c90b5c9dde11aac4173ac.jpg)  \n\nHere N is the total number of pixels and C is the color channel. Standard deviation (SD) is the average of the squared differences from the Mean shown in equation 5.  \n\n$$\nS D=\\sqrt{\\frac{1}{N}\\sum_{k=1}^{N}(C_{k}-\\mu)^{2}}\n$$  \n\nwhere $\\mathbf{N}$ is the total number of pixels and $\\mu$ is the mean and $\\mathrm{C_{k}}$ is the specific Color channel.  \n\nSkewness calculates that how much the Asymmetry of the probability distribution of some are given real-valued random variable about the mean. The mathematical description is given below in equation 6.  \n\n$$\nS k=\\sqrt[3]{\\frac{1}{N}\\sum_{k=1}^{N}(C_{k}-\\mu)^{3}}\n$$  \n\nwhere $\\mathbf{N}$ is the total number of pixels and $\\mu$ is the mean and $\\mathrm{C_{k}}$ is the specific Color channel.  \n\nVariance is the variation of the color distribution shown in equation 7.  \n\n$$\nV a r=\\frac{1}{N}\\sum_{k=1}^{N}(C_{k}-\\mu)^{2}\n$$  \n\nFor an accurate diagnosis of skin cancer, each discriminative feature is extracted individually. GLCM assesses the global texture of the image whereas, LBP provides the texture analysis of patches within the image. Their combination forms a distinctive feature descriptor providing an optimal measure of the texture. Alongside lesion is also characterized by its color. Thus, the textural along with the color features listed with the description in Table 1, are combined to form a hybrid super feature vector. The feature-length as a result of the concatenation of the individual features are summed up to a total feature length of 294.  \n\n# D. CLASSIFICATION  \n\nAfter segmentation and feature extraction, the hybrid feature vector is then provided to classifiers to identify the melanoma and nevus. Different classification algorithms are trained and tested at their default parameter settings to achieve high accuracy as discussed below.  \n\n# 1) SUPPORT VECTOR MACHINES  \n\nSupport Vector Machines (SVM) consists of a set of supervised learning methods originally discussed by Vapnik in 1963. This technique is used to reduce the classification error and also maximizes the geometric boundary that separates the class values. They are also known as maximum margin classifier. In N-dimensional feature space, each data point is plotted at its respective coordinates. The classifier works by finding the right hyper-plane to separate the data points into required classes. Once the hyperplane is determined, the testing samples are predicted to be on either side of the plane. The hyperplane can be characterized as  \n\nTABLE 1. Feature Description Table.   \n\n![](images/12d74a3da498e2287b4fba1e4a3bea300f6747ca635f9c36b61fde18faace36e.jpg)  \n\n![](images/becaef5ed654dff34ac6a65a13d2941d3e04604ff1a98be26e7f9eeb6c9ef2c9.jpg)  \nFIGURE 6. SVM Classifier Model.  \n\n$$\n\\mathbf{w.X+b=0}\n$$  \n\nHere $\\mathbf{X}$ denotes the N-dimensional input vector, w is the vector weight defined as $\\up w=\\up w_{1}$ , $\\mathbf{W}2$ , $\\mathbf{W}3\\cdot\\ldots\\ldots\\mathbf{W}_{\\mathrm{n}}$ , while $\\mathbf{b}$ is the model bias function. As we have two classes melanoma and nevus, several decision boundaries may occur. SVM identifies the decision boundary, the hyper-plane having the most extreme separation from the two classes. The proposed SVM utilizes linear kernel with margin constant C as 1.  \n\nThe hyperplane dotted line and the binary classes $+1$ and $^{-1}$ is shown in figure 6.  \n\nClassification using SVM is a supervised machine learning approach that needs appropriate training on larger datasets for binary classes. SVM selects the values of w and b from the training samples. SVM identifies a hyperplane for maximum separation between true and false training examples. From the below equations the hyperplane $\\mathrm{H}$ over the training data samples are represented as  \n\n$$\n\\begin{array}{c}{{X_{i}*w+b\\leq1,y_{i}=-1}}\\ {{X_{i}*w+b\\geq1,y_{i}=+1}}\\end{array}\n$$  \n\nFrom equation 9 and 10, we get  \n\n$$\ny_{i}\\left(x_{i},w+b\\right)-1\\geq0,\\forall i\n$$  \n\nFrom the above equations, it is observed that all the trained samples may occur on both sides of the hyperplane. As the model is trained, the solution of the decision problem is found by evaluating the sign of the $\\mathrm{y_{i}}$ with the coefficient vector w.  \n\nClassifiers take these textural and color features as input for classification. Detection and classification of melanoma is a binary classification problem with discrete values of the data-points calculated over a dataset of an adequate number of images making it suitable for SVM. The accuracy of the classifier is calculated mapping the classified results with the ground truth data.  \n\n# 2) K-NEAREST NEIGHBOR  \n\nK-nearest neighbors (KNN) is a machine learning algorithm that keeps all training values and classifies new ones based on a similarity measure as given in the equation below.  \n\n$$\nd={\\sqrt{\\sum_{i=1}^{K}(x-y_{i})^{2}}}\n$$  \n\nTo find the distance between the attributes of training and testing data-points Euclidean distance is used. Here $\\mathbf{X}$ and y denote the attributes of testing and training data-points respectively, while i represents the ith number of y instance compared with $\\mathbf{X}$ out of a total number, K. Majority votes for the class out of K instances based on minimum distance are used to find the respective class. For binary classification, the value of $\\mathrm{K}$ must be set to an odd number to remove the possibility of a draw between the decisions. With the training data-points being plotted in the feature space, neighboring testing samples are classified as melanoma or nevus based on the minimum distance from the majority of K samples. Our proposed method utilizes the KNN with a number of neighbors as 1 and Euclidean distance is used. The main disadvantage of the KNN algorithm is its complexity as it consumes all resources for computing the values of the features.  \n\n# 3) NAIVE BAYESIAN  \n\nThis classification algorithm is based on Bayes’ theorem with the independence assumption between predictors. It is simple to build a Naïve Bayesian model as there is no iterative parameters estimation. It is very much useful when the number of images is larger. Due to its simple nature, Naïve Bayesian algorithm gives better results in solving sophisticated problems. The posterior probability is calculated by Bayes theorem. Naive Bayes classifier adopts that the result of the value of a predictor $\\mathbf{\\rho}(\\mathbf{x})$ on a given class (c) is independent of the values of other predictors. This statement is known as class conditional independence.  \n\n$$\nP\\left(c|x\\right)=\\frac{P\\left(x|c\\right)P(c)}{P(x)}\n$$  \n\nwhere  \n\n• The posterior probability of given class is $\\mathrm{P}(\\mathrm{c}/\\mathrm{x})$ • The prior probability of class is $\\mathrm{P}(\\mathrm{c})$ .  \n\n![](images/602de07b133880642a17ca797979742f9921dadf9830d4d2812df4ddeb0d0f2f.jpg)  \nFIGURE 7. Confusion Matrix.  \n\n• The probability of predictor given class $\\mathrm{{P}(\\mathbf{x}|\\mathbf{c})}$ .   \n• The predictor prior probability is $\\mathrm{P}(\\mathbf{x})$ .  \n\nNaïve Bayesian classifies the input hybrid feature vector x into melanoma or nevus with the assumption that the individual attributes are independent to one another. The input vector is classified based on the higher posterior probability for the two classes i.e. melanoma and nevus.  \n\n# 4) DECISION TREES  \n\nA decision tree is the flowchart representation of attributes in the form of a tree where each attribute acts as a node. The tree is formed with the best attribute as the root node and then passes through each attribute following the way down to the leaf node i.e. its respective class. This results in the formulation of decision rules against which the test samples or records are classified. In decision trees, the instances are classified by sorting them using a top-down approach from root to a leaf node. First, we take the root node of the tree and classify each instance. Then test each attribute of this node and moving down to remaining of the tree branch corresponding to the attribute value. The same process is repeated for the sub-tree rooted at the new node.  \n\n# IV. DATASETS AND EVALUATION METRICS  \n\nWe have used the DermIS image database [16] in our research work. The database consists of 146 malignant Melanoma and 251 Nevus images. The results obtained are briefly discussed in the form of classification performance. Four possible outcomes occur for binary class classification i.e. true positive, true negative, false positive and false negative.  \n\nA true positive represents when melanoma is classified by the system as a melanoma while true negative happens when a lesion belongs to nevus and classified as a nevus. On the other way, a predicted false positive value happens when a lesion is not melanoma and it is classified as melanoma while a false negative happens when a certain lesion is melanoma and is classified as nevus shown in figure 7.  \n\nThe performance of binary class classification can be calculated in terms of its sensitivity, specificity and accuracy quantifying its performance to false positive (FP) and false negative (FN) instances.  \n\n• Sensitivity: The rate of positives values that are efficiently recognized by the classifier.  \n\n$$\nS e n={\\frac{T P}{T P+F N}}\n$$  \n\n• Specificity: The ratio of negatives that are correctly identified by the classifier  \n\n$$\nS p e c=\\frac{T N}{T N+F P}\n$$  \n\n• Accuracy: The proportion of the number of correctly identified cases to the total number of test cases as,  \n\n$$\nA C C={\\frac{T P+T N}{(T P+T N+F P+F N)}}\n$$  \n\n• Precision: The proportion of predicted cases that are related to malignancy.  \n\n$$\nP r e c=\\frac{T P}{T P+F P}\n$$  \n\n# $\\mathbf{\\{\\Psi\\}}$ RESULTS AND DISCUSSION  \n\nThe segmented region in the skin cancer images describes cancer affected lesion. This cancerous lesion needs to be  \n\nextracted from the background skin information. K-Means clustering using centroid selection has been utilized for the segmentation of the input images based on the variation in the intensities with the value of K is set as 2.  \n\nThe clusters formed over the images are then used for further processing to accurately detect the melanoma cancer. Figure 8 shows the segmentation results using the improved K-Mean technique. Ground truth data is marked on to the images within our dataset with the help of physician and skin cancer affected region and boundaries are identified. The K-Mean segmentation technique extracts the ROIs from the images and segments out cancer affected region from the background. To measure the segmentation accuracy of a system, the percentage of overlap between the segmented area and the ground truth information is calculated. It is achieved through the Jaccard Index. For an image with segmented region A, Jaccard Index of similarity with the ground truth image B is calculated as,  \n\n$$\nJ(A,B)={\\frac{|A\\cap B|}{|A\\cup B|}}\n$$  \n\nThe total segmentation accuracy of the proposed work is 0.94 in comparison to the utilization of standard K-Mean giving an overall index as 0.86. Using improved K-Mean $94\\%$ of the images were accurately segmented. The shortcoming in detection is mainly due to mismatch of coordinates at borders due to low contrast between the skin color and cancerous region. The smooth transition of Melanoma affected region into the skin blurs the transition between the two intensities leading to misleading detection results.  \n\n![](images/ec7d68c859c7f1e596c8540c490f7ed575c8b4b485bb78a4db761067d51ef1b0.jpg)  \nFIGURE 8. a) Original Image b) Segmented Image c) Ground Truth.  \n\n![](images/263d1e1d8a8e2b49174229dff7efbb702f27a83cec36d9aec294cb23040ba577.jpg)  \nFIGURE 9. Image-wise segmentation accuracy comparison using standard K-Mean and improved K-Mean.  \n\n# A. COMPARATIVE ANALYSIS WITH SIMPLE K-MEAN  \n\nThe $\\mathrm{k}$ -mean clustering algorithm is utilized to extract the region of interest against which ground truth labels are specified. It is of high interest that all the ground truth components are extracted in the process of segmentation. A comparative analysis has been drawn between the segmentation accuracy of the proposed improved K-mean technique and the standard K-Mean. Both segmentation techniques have been utilized over the images within our dataset and segmentation accuracy has been recorded. It has been observed that the K-Mean technique using centroid selection proved to segment the information more accurately. Image-wise segmentation accuracy achieved over the two versions of K-Mean is shown in figure 9. It has been observed that for images with low the contrast between the foreground and background information segmentation results of proposed K-Mean seem to outperform the standard one.  \n\nOut of the $100\\%$ detection of images in our dataset, proposed improved K-Mean showed segmentation accuracy of $94\\%$ as compared to the standard algorithm performing at $86\\%$ .  \n\nStandard K-Mean technique works on the random selection of intensity levels as centroids for clustering of data points.  \n\nInitial centroid selection plays a key role in the performance of the algorithm. Selection of centroids through a proper mechanism ensures the initialization of centroids with significant intensity variation. It not only improves the segmentation results of the input data but also expedites the process by completing it in a fewer number of iterations. The lesser number of iterations results in an overall low computation cost of our proposed system. A comparative analysis has been drawn between the computational time (CT) of segmentation utilizing standard K-Mean and improved K-Mean as shown in figure 10. It has been observed that the improved K-Mean segmented images in a fewer number of iterations resulting in a low average computational time of 166 milliseconds in comparison to 196 milli-seconds for standard K-Mean utilization.  \n\n![](images/33b4fddb1a36b338c174f2f674ff4e57429a7a57be426a9f7cf06eb95b1ea502.jpg)  \nFIGURE 10. Image wise Computational Time Analysis of standard K-Mean and Improved K-Mean.  \n\n# B. COMPARATIVE ANALYSIS WITH SEPARATE AND HYBRID FEATURE  \n\nMultiple features are utilized to test our proposed system for accurate classification of skin cancer and the respective results have been recorded and given in Table 2. The table lists the classification accuracy of our system on individual features along with the classification results of the hybrid feature vector. The accuracy of our system over the classification of listed features depicts the significance of those features for accurate classification. Results show that the individual features distinctive they may be, however, showed no significant contribution inaccuracy of the system. Whereas, their combination showed a significant improvement in the classification results. It also clearly shows that the hybridization of features tends to create a more distinctive decision boundary between the two classes.  \n\nColor is an important feature that a human perceives while viewing or examining an image. Therefore, color can be proved as a crucial feature for distinguishing the Melanoma with Nevus. Hence, the utilization of color features extricates imperative features. Statistical results showed that with color features only, the classification of melanoma has been achieved $77\\%$ accurately. On the other hand, textural features capture pertinent data for Melanoma identification. Global and textural features of GLCM and LBP respectively showed low levels of accuracy however their combined formation results in a better distinctive textural information.  \n\nOur proposed method utilizes the distinctive ability of color features along with the combination of local and global textural features resulting in a hybrid feature vector showing promising classification results.  \n\nTABLE 2. Comparison of individual and combined features classification.   \n\n![](images/d35a012773606c0466ec925c17f76f7c03a4cbd6d3c6ff94b84f8646f4e63337.jpg)  \n\nVisual representation of local texture and statistical values of global textural information is shown in figure 11.  \n\nAs observed from Table 2 individual classifier accuracy underperforms with respect to combined features. GLCM features achieve accuracy of $62\\%$ with KNN as a classifier.  \n\nLBP features perform better results compared to GLCM achieving $70\\%$ accuracy.  \n\nAs discussed, color is important and decrementing factor amongst all type of skin cancer, color feature achieves $77\\%$ accuracy. From the subsequent experiments, it is observed by combining GLCM, LBP, and color the level of accuracy achieved by our system is $96\\%$ .  \n\n# C. COMPARISON WITH OTHER METHODS  \n\nThe binary-class classification has been of significant interest in the research community. Many different methodologies and techniques have been proposed to accurately classify a lesion into cancerous or non-cancerous.  \n\nA comparative analysis has been made in Table III with only those existing methods that have utilized the same dataset as ours (DermIS) providing a benchmark for our proposed methodology. Robert Amelard et al. [11], utilized the conventional approach of ABCD for classifying the lesion as melanoma and nevus. They used images DermIS and Dermquest databases and were able to identify the test images $94\\%$ accurately. In another study Robert Amelard et al. [28] with same dataset and High-Level Intuitive Features (HLIFs)  \n\n![](images/287e11c8405ad1c290369b445ac66b710b8fc08761d8b016ae78e217f5263232.jpg)  \nFIGURE 11. (a) Input Segmented Image (b) Visual representation of LBP (c) Plot of statistical features of GLCM.  \n\nwith ABCD features archives $86\\%$ accuracy. Ebtihal Almansour in [53] utilized Hybrid features as a combination of GLCM, LBP and color features to classify melanoma. The technique was evaluated over the DermIS dataset with an accuracy of $90\\%$ using SVM. The technique gave promising results however did not utilize the whole dataset and accuracy was calculated over only 227 images. Shoieb et. al. in [47] proposed a deep learning-based feature extraction through CNN network and classifying them through SVM. The algorithm was tested over multiple datasets however gave an overall accuracy of $93.75\\%$ over 69 images acquired through DermIS dataset. M. Ali et al. in [37] suggested a technique to identify cancer affected skin lesions using their textural, color-based and morphological features. The respective features were extracted from images acquired through DermIS dataset giving an overall classification accuracy of $80\\%$ . The proposed method was tested over 50 images. Rebecca et al in [26] proposed ABCD feature-based method to classify melanoma using K-Nearest Neighbor algorithm. The technique was tested over a few images of DermIS dataset giving an accuracy of $89\\%$ . Considering the above methodologies found in literature and the information used as a baseline, existing techniques left significant room to identify the most distinctive features for accurate detection and classification of melanoma. The also showed the need to test the devised algorithm over an adequate number of images adding more confidence to the evaluated results. It also left with an adequate number of images. To address the aforementioned shortcomings, a novel technique was developed utilizing K-mean clustering using centroid selection for efficient and better segmentation. Texture and color features were extracted from the skin lesion. The features showed discriminative significance when utilized in combination with $96\\%$ classification accuracy.  \n\nTABLE 3. Comparative analysis with existing techniques.   \n\n![](images/dc98b5eb340172d6f0ee15ecad1d6ea427d22b04e11a74e63fdbd8b98dbc4795.jpg)  \n\n# VI. CONCLUSION  \n\nIn this research paper, we presented an intelligent system for classification of skin cancer into melanoma and nevus. It is observed that major problem that causes the misclassification is lesion detection and segmentation. The K-mean clustering technique using centroid selection is used to extract the ROI from the cancer image more accurately and efficiently. Textural and color features extraction techniques are used to obtain best-suited features for classification. For texture features, GLCM and LBP features are combined with the color features to achieve a high classification accuracy of $96\\%$ . In this way, our proposed technique has been able to classify skin cancer images into melanoma and nevus more accurately and efficiently. The effectiveness and performance of the proposed approach are validated on DERMIS image dataset.  \n\n# REFERENCES  \n\n[1] M. Aleem, N. Hameed, A. Anjum, and F. Hameed, ‘‘m-Skin Doctor: A mobile enabled system for early melanoma skin cancer detection using support vector machine,’’ eHealth, vol. 2, pp. 468–475, Dec. 2016.  \n\n[2] B. Kong, S. Sun, X. Wang, Q. Song, and S. Zhang, Invasive Cancer Detection Utilizing Compressed Convolutional Neural Network and Transfer Learning (Lecture Notes in Computer Science). Cham, Switzerland: Springer, 2018.   \n[3] Q. Li, L. Chang, H. Liu, M. Zhou, Y. Wang, and F. Guo, ‘‘Skin cells segmentation algorithm based on spectral angle and distance score,’’ Opt. Laser Technol., vol. 74, pp. 79–86, Nov. 2015.   \n[4] R. Kasmi and K. Mokrani, ‘‘Classification of malignant melanoma and benign skin lesions: Implementation of automatic ABCD rule,’’ IET Image Process., vol. 10, no. 6, pp. 448–455, Jun. 2016.   \n[5] R. J. Friedman, D. S. Rigel, and A. W. Kopf, ‘‘Early detection of malignant melanoma: The role of physician examination and self-examination of the skin,’’ CA, Cancer J. Clinician, vol. 35, no. 3, pp. 130–151, May/Jun. 1985.   \n[6] H. Ganster, P. Pinz, R. Rohrer, E. Wildling, M. Binder, and H. Kittler, ‘‘Automated melanoma recognition,’’ IEEE Trans. Med. Imag., vol. 20, no. 3, pp. 233–239, Mar. 2001.   \n[7] G. Betta, G. Di Leo, G. Fabbrocini, A. Paolillo, and M. Scalvenzi, ‘‘Automated application of the ‘‘7-point checklist’’ diagnosis method for skin lesions: Estimation of chromatic and shape parameters,’’ in Proc. IEEE Instrum. Meas. Technol. Conf., vol. 3, May 2005, pp. 1818–1822.   \n[8] G. Argenziano, C. Catricalá, M. Ardigo, P. Buccini, P. De Simone, L. Eibenschutz, A. Ferrari, G. Mariani, V. Silipo, I. Sperduti, and I. Zalaudek, ‘‘Seven-point checklist of dermoscopy revisited,’’ Brit. J. Dermatology, vol. 164, no. 4, pp. 785–790, Apr. 2011.   \n[9] I. Zalaudek, G. Argenziano, H. P. Soyer, R. Corona, F. Sera, A. Blum, R. P. Braun, H. Cabo, G. Ferrara, A. W. Kopf, D. Langford, S. W. Menzies, G. Pellacani, K. Peris, and S. Seidenari, ‘‘Three-point checklist of dermoscopy: An open Internet study,’’ Brit. J. Dermatology, vol. 154, no. 3, pp. 431–437, Mar. 2006.   \n[10] P. Mohanaiah, P. Sathyanarayana, and L. Gurukumar, ‘‘Image texture feature extraction using GLCM approach,’’ Int. J. Sci. Res. Publ., vol. 3, no. 5, pp. 1–5, 2013.   \n[11] R. Amelard, J. Glaister, A. Wong, and D. A. Clausi, ‘‘High-level intuitive features (HLIFs) for intuitive skin lesion description,’’ IEEE Trans. Biomed. Eng., vol. 62, no. 3, pp. 820–831, Mar. 2015.   \n[12] D. O. T. Bruno, M. Z. D. Nascimento, R. P. Ramos, V. R. Batista, L. A. Neves, and A. S. Martins, ‘‘LBP operators on curvelet coefficients as an algorithm to describe texture in breast cancer tissues,’’ Expert Syst. Appl., vol. 55, pp. 329–340, Aug. 2016.   \n[13] N. R. Abbasi, H. M. Shaw, D. S. Rigel, R. J. Friedman, W. H. McCarthy, I. Osman, A. W. Kopf, and D. Polsky, ‘‘Early diagnosis of cutaneous melanoma: Revisiting the ABCD criteria,’’ J. Amer. Med. Assoc., vol. 292, no. 22, pp. 2771–2776, Dec. 2004.   \n[14] R. H. Johr, ‘‘Dermoscopy: Alternative melanocytic algorithms-the ABCD rule of dermatoscopy, Menzies scoring method, and 7-point checklist,’ Clin. Dermatol., vol. 20, no. 3, pp. 240–247, May/Jun. 2002.   \n[15] H. Kittler, M. Seltenheim, M. Dawid, H. Pehamberger, K. Wolff, and M. Binder, ‘‘Morphologic changes of pigmented skin lesions: A useful extension of the ABCD rule for dermatoscopy,’’ J. Amer. Acad. Dermatol., vol. 40, no. 4, pp. 558–562, Apr. 1999.   \n[16] E. Almansour and M. A. Jaffar, ‘‘Classification of Dermoscopic skin cancer images using color and hybrid texture features,’’ Int. J. Comput. Sci. Netw. Secur., vol. 16, no. 4, pp. 135–139, Apr. 2016.   \n[17] F. Xie, H. Fan, Y. Li, Z. Jiang, R. Meng, and A. C. Bovik, ‘‘Melanoma classification on dermoscopy images using a neural network ensemble model,’’ IEEE Trans. Med. Imag., vol. 36, no. 3, pp. 849–858, Mar. 2016.   \n[18] M. B. Bonab, S. Z. M. Hashim, A. K. Z. Alsaedi, and U. R. Hashim, ‘‘Modified K-means combined with artificial bee colony algorithm and differential evolution for color image segmentation,’’ in Computational Intelligence in Information Systems, vol. 331. Cham, Switzerland: Springer, pp. 221–231, 2015.   \n[19] M. Silveira, J. C. Nascimento, J. S. Marques, A. R. S. Marcal, T. Mendonca, S. Yamauchi, J. Maeda, and J. Rozeira, ‘‘Comparison of segmentation methods for melanoma diagnosis in dermoscopy images,’’ IEEE J. Sel. Topics Signal Process., vol. 3, no. 1, pp. 35–45, Feb. 2009.   \n[20] F. Nachbar, ‘‘The ABCD rule of dermatoscopy: High prospective value in the diagnosis of doubtful melanocytic skin lesions,’’ J. Amer. Acad. Dermatol., vol. 30, no. 4, pp. 551–559, Apr. 1994.   \n[21] M. Keefe, D. C. Dick, and R. A. Wakeel, ‘‘A study of the value of the seven-point checklist in distinguishing benign pigmented lesions from melanoma,’’ Clin. Exp. Dermatol., vol. 15, no. 3, pp. 167–171, May 1990.   \n[22] M. F. Healsmith, J. F. Bourke, J. E. Osborne, and R. A. C. Graham-Brown, ‘‘An evaluation of the revised seven-point checklist for the early diagnosis of cutaneous malignant melanoma,’’ Brit. J. Dermatol., vol. 130, no. 1, pp. 48–50, Jan. 1994.   \n[23] H. P. Soyer, G. Argenziano, I. Zalaudek, R. Corona, F. Sera, R. Talamini, F. Barbato, A. Baroni, L. Cicale, A. Di Stefani, P. Farro, L. Rossiello, E. Ruocco, and S. Chimenti, ‘‘Three-point checklist of dermoscopy. A new screening method for early detection of melanoma,’’ Dermatology, vol. 208, no. 1, pp. 27–31, 2004.   \n[24] A. Masood and A. A. Al-Jumaily, ‘‘Computer aided diagnostic support system for skin cancer: A review of techniques and algorithms,’’ Int. J. Biomed. Imag., vol. 2013, Oct. 2013, Art. no. 323268.   \n[25] S. Demyanov, R. Chakravorty, M. Abedini, A. Halpern, and R. Garnavi, ‘‘Classification of dermoscopy patterns using deep convolutional neural networks,’’ in Proc. IEEE 13th Int. Symp. Biomed. Imag. (ISBI), Apr. 2016, pp. 364–368.   \n[26] R. Moussa, F. Gerges, C. Salem, R. Akiki, O. Falou, and D. Azar, ‘‘Computer-aided detection of Melanoma using geometric features,’’ in Proc. 3rd Middle East Conf. Biomed. Eng. (MECBME), Oct. 2016, pp. 125–128.   \n[27] G. Argenziano, H. P. Soyer, S. Chimenti, R. Talamini, R. Corona, F. Sera, M. Binder, L. Cerroni, G. De Rosa, G. Ferrara, and R. Hofmann-Wellenhof, ‘‘Dermoscopy of pigmented skin lesions: Results of a consensus meeting via the Internet,’’ J. Amer. Acad. Dermatol., vol. 48, no. 5, pp. 679–693, May 2003.   \n[28] R. Amelard, A. Wong, and D. A. Clausi, ‘‘Extracting high-level intuitive features (HLIF) for classifying skin lesions using standard camera images,’’ in Proc. 9th Conf. Comput. Robot Vis., May 2012, pp. 396–403.   \n[29] M. E. Celebi, H. A. Kingravi, B. Uddin, H. Iyatomi, Y. A. Aslandogan, W. V. Stoecker, and R. H. Moss, ‘‘A methodological approach to the classification of dermoscopy images,’’ Comput. Med. Imag. Graph., vol. 31, no. 6, pp. 362–373, Sep. 2007.   \n[30] R. J. Stanley, W. V. Stoecker, and R. H. Moss, ‘‘A relative color approach to color discrimination for malignant melanoma detection in dermoscopy images,’’ Skin Res. Technol., vol. 13, no. 1, pp. 62–72, Feb. 2007.   \n[31] C. Barata, M. Ruela, M. Francisco, T. Mendonça, and J. S. Marques, ‘‘Two systems for the detection of melanomas in dermoscopy images using texture and color features,’’ IEEE Syst. J., vol. 8, no. 3, pp. 965–979, Sep. 2014.   \n[32] P. Rubegni, G. RubegniCevenini, M. Burroni, R. Perotti, G. Dell’Eva, P. Sbano, C. Miracco, P. Luzi, P. Tosi, P. Barbini, and L. Andreassi, ‘‘Automated diagnosis of pigmented skin lesions,’’ Int. J. Cancer, vol. 101, no. 6, pp. 576–580, Oct. 2002.   \n[33] R. B. Oliveira, J. P. Papa, A. S. Pereira, and J. M. R. S. Tavares, ‘‘Computational methods for pigmented skin lesion classification in images: Review and future trends,’’ Neural Comput. Appl., vol. 29, no. 3, pp. 613–636, Feb. 2018.   \n[34] I. Stanganelli, A. Brucale, L. Calori, R. Gori, A. Lovato, S. Magi, B. Kopf, R. Bacchilega, V. Rapisarda, A. Testori, P. A. Ascierto, E. Simeone, and M. Ferri, ‘‘Computer-aided diagnosis of melanocytic lesions,’’ Anticancer Res., vol. 25, no. 6C, pp. 4577–4582, Nov./Dec. 2005.   \n[35] H. Iyatomi, H. Oka, M. E. Celebi, K. Ogawa, G. Argenziano, H. P. Soyer, H. Koga, T. Saida, K. Ohara, and M. Tanaka, ‘‘Computer-based classification of dermoscopy images of melanocytic lesions on acral volar skin,’’ J. Investigative Dermatol., vol. 128, no. 8, pp. 2049–2054, Aug. 2008.   \n[36] Q. Abbas, M. E. Celebi, and I. Fondón, ‘‘Computer-aided pattern classification system for dermoscopy images,’’ Skin Res. Technol., vol. 18, no. 3, pp. 278–289, Aug. 2012.   \n[37] M. A. Farooq, M. A. M. Azhar, and R. H. Raza, ‘‘Automatic lesion detection system (ALDS) for skin cancer classification using SVM and neural classifiers,’’ in Proc. IEEE 16th Int. Conf. Bioinf. Bioeng. (BIBE), Oct./Nov. 2016, pp. 301–308.   \n[38] J. F. Alcon, C. Ciuhu, W. T. Kate, A. Heinrich, N. Uzunbajakava, G. Krekels, D. Siem, and G. de Haan, ‘‘Automatic imaging system with decision support for inspection of pigmented skin lesions and melanoma diagnosis,’’ IEEE J. Sel. Topics Signal Process., vol. 3, no. 1, pp. 14–25, Feb. 2009.   \n[39] I. Giotis, N. Molders, S. Land, M. Biehl, M. F. Jonkman, and N. Petkov, ‘‘MED-NODE: A computer-assisted melanoma diagnosis system using non-dermoscopic images,’’ Expert Syst. Appl., vol. 42, no. 19, pp. 6578–6585, Nov. 2015.   \n[40] A. Safi, Computer–Aided Diagnosis of Pigmented Skin Dermoscopic Images (Lecture Notes in Computer Science: Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 7075. Berlin, Germany: Springer, 2012, pp. 105–115.   \n[41] I. Maglogiannis and D. I. Kosmopoulos, ‘‘Computational vision systems for the detection of malignant melanoma,’’ Oncol. Rep., vol. 15, no. 4, pp. 1027–1032, Apr. 2006.   \n[42] D. Ruiz, V. Berenguer, A. Soriano, and B. Sánchez, ‘‘A decision support system for the diagnosis of melanoma: A comparative approach,’’ Expert Syst. Appl., vol. 38, no. 12, pp. 15217–15223, Nov./Dec. 2011.   \n[43] A. Masood and A. Al-jumaily, ‘‘Differential evolution based advised SVM for histopathalogical image analysis for skin cancer detection,’’ in Proc. 37th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC), pp. 781–784, Aug. 2015.   \n[44] S. Wu, Z. Gao, Z. Liu, J. Luo, H. Zhang, and S. Li, Direct Reconstruction of Ultrasound Elastography Using an End-to-End Deep Neural Network (Lecture Notes in Computer Science: Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Cham, Switzerland: Springer, 2018.   \n[45] B. Kong, Y. Zhan, M. Shin, T. Denny, and S. Zhang, Recognizing EndDiastole and End-Systole Frames Via Deep Temporal Regression Network (Lecture Notes in Computer Science: Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). Cham, Switzerland: Springer, 2016.   \n[46] U.-O. Dorj, K.-K. Lee, J.-Y. Choi, and M. Lee, ‘‘The skin cancer classification using deep convolutional neural network,’’ Multimedia Tools Appl., vol. 77, no. 8, pp. 9909–9924, Apr. 2018.   \n[47] D. A. Shoieb, S. M. Youssef, and W. M. Aly, ‘‘Computer-aided model for skin diagnosis using deep learning,’’ J. Image Graph., vol. 4, no. 2, pp. 122–129, Dec. 2019.   \n[48] H. Lee and Y.-P. P. Chen, ‘‘Image based computer aided diagnosis system for cancer detection,’’ Expert Syst. Appl., vol. 42, no. 12, pp. 5356–5365, Jul. 2015.   \n[49] R. M. Haralick, K. Shanmugam, and I. Dinstein, ‘‘Textural features for image classification,’’ IEEE Trans. Syst., Man, Cybern., vol. SMC-3, no. 6, pp. 610–621, Nov. 1973.   \n[50] T. Ojala, M. Pietikäinen, and D. Harwood, ‘‘A comparative study of texture measures with classification based on featured distributions,’’ Pattern Recognit., vol. 29, no. 1, pp. 51–59, 1996.   \n[51] S. Sabbaghi, M. Aldeen, and R. Garnavi, ‘‘A deep bag-of-features model for the classification of melanomas in dermoscopy images,’’ in Proc. 38th Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. (EMBC), Aug. 2016, pp. 1369–1372.   \n[52] V. Rajinikanth, S. C. Satapathy, N. Dey, S. L. Fernandes, and K. S. Manic, ‘‘Skin melanoma assessment using Kapur’s entropy and level set—A study with bat algorithm,’’ in Smart Innovation, Systems and Technologies. Singapore: Springer, 2019.   \n[53] M. A. J. E. Almansour, ‘‘Classification of dermoscopic skin cancer images using color and hybrid texture features title,’’ Int. J. Comput. Sci. Netw. Secur., vol. 16, no. 4, pp. 135–139, Apr. 2016.  \n\nMUHAMMAD QASIM KHAN received the master’s degree in computer science from the University of Arid Agriculture, Rawalpindi, Pakistan, in 2011. He is currently pursuing the Ph.D. degree with the Department of Computer Science and Software Engineering, International Islamic University, Islamabad, Pakistan. His current research interests include image processing and machine learning.  \n\n![](images/4c515585708fbdf8c8786b54e0c43df1c46d10bd963154efb5cfdd6b78d69098.jpg)  \n\n![](images/859a28b38a62fb60f9906e9987526532131da0bf861db145ec35549182eba4d6.jpg)  \n\nAYYAZ HUSSAIN is currently an Associate professor and the Head of the Department of Computer Science and Software Engineering, International Islamic University, Islamabad, Pakistan. His research interests include image processing, pattern recognition, machine learning and data mining, and computational intelligence techniques.  \n\n![](images/2abdb72f4e90934a95253238efb88be7641153061de674e65ac37a2e5a3637fa.jpg)  \n\nMUAZZAM MAQSOOD received the Ph.D. degree from UET Taxila, in 2017. He has been serving as an Assistant Professor with COMSATS University Islamabad at Attock, Pakistan. His research interests include machine learning, recommender systems, and image processing.  \n\n![](images/88e13da6c56dbddb7c2a6c05792bd7a5d854e576fcf4e2196d6bf78c1e75f500.jpg)  \n\nSAEED UR REHMAN is currently an Assistant Professor with COMSATS University Islamabad at Attock, Pakistan.  \n\n![](images/be37112532b91045c73ae1d93d04a8a6a177f821341bbd3c370d69098cca71d6.jpg)  \n\nKASHIF MEHMOOD has been serving as a Lecturer with COMSATS University Islamabad at Attock, Pakistan.  \n\n![](images/c463c92925c27b2a8ba56fe8e06db83961030e059302da1d0ed673756000954b.jpg)  \n\nUMAIR KHAN is a Graduate Student of Computer Systems Engineering Program with the University of Engineering and Technology (UET), Peshawar, Pakistan, and he is currently pursuing the M.S. program in Computer System Engineering with UET. He is currently a Research Associate with COMSATS University Islamabad at Attock. His research interests include the advanced concepts of image processing and machine learning.  \n\n![](images/c783f9f3856bd92fe7b78fe910fd496e42fb97b2fbaf47815b29220faea70e76.jpg)  \n\nMUAZZAM A. KHAN has been serving as an Associate Professor with the Department of Computing, SEECS, National University of Sciences and Technology, Islamabad, Pakistan. His research interest include WSN, the IoT, VANETS, and data and network security.  "}