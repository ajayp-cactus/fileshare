{"md_content":"# Artificial intelligence in the detection of skin cancer: State of the art  \n\nMichat Strzelecki, PhDa,\\*, Marcin Kociotek, PhDa, Maria Strakowska, PhDa, Michat Koztowski, PhDb, Andrzej Grzybowski, MD, PhD, Piotr M. Szczypinski, PhD  \n\nInstitute of Electronic, Lod UniversityofTechnology,Ld, Poland   \nb Department of Mechatronics and Technical and IT Education, Faculty of Technical Science, University of Warmia and   \nMazury, Olsztyn, Poland   \nc Institute for Research in Ophthalmology, Foundation for Ophthalmology Development, Poznan, Poland  \n\nAbstract The incidence of melanoma is increasing rapidly. This cancer has a good prognosis if detected early. For this reason, various systems of skin lesion image analysis, which support imaging diagnostics of this neoplasm, are developing very dynamically. To detect and recognize neoplastic lesions, such systems use various artificial intelligence (AI) algorithms. This area of computer science applications has recently undergone dynamic development, abounding in several solutions that are effective tools supporting diagnosticians in many medical specialties. In this contribution, a number of applications of different classes of AI algorithms for the detection of this skin melanoma are presented and evaluated. Both classic systems based on the analysis of dermatoscopic images as well as total body systems, enabling the analysis of the patient’s whole body to detect moles and pathologic changes, are discussed. These increasingly popular applications that allow the analysis of lesion images using smartphones are also described. The quantitative evaluation of the discussed systems with particular emphasis on the method of validation of the implemented algorithms is presented. The advantages and limitations of AI in the analysis of lesion images are also discussed, and problems requiring a solution for more effective use of AI in dermatology are identified.  \n\n$\\copyright$ 2024 Elsevier Inc. All rights reserved.  \n\n# Introduction  \n\nSkin cancer ranks among the most common types of cancer, and it is recognized for its highly malignant nature. Its prevalence has outpaced that of nearly all other cancers, with annual rates rising by approximately $3\\%$ to $7\\%$ in fairskinned populations over the past few decades.1,2 Consequently, there is a pressing need to conduct extensive screening tests to detect skin cancer at early stages. To facilitate this, it is crucial to automate the diagnosis of skin moles through the utilization of image analysis and computational artificial intelligence (AI) techniques.  \n\nEarly detection of melanoma, the most aggressive type of skin cancer, has a significant impact on patient prognosis. It increases the chances of successful treatment, prevents disease progression, and reduces the risk of life-threatening complications. Early diagnosis allows for less invasive surgical procedures, shorter recovery time, and effective adjuvant therapies. Regular skin examinations and prompt intervention are crucial for improving patient outcomes. 3  \n\nArtificial intelligence has emerged as a transformative force in various fields of human activity, including medical diagnostics. In dermatology, the application of AI techniques has garnered significant attention, leading to progress in automated diagnosis and management of skin conditions.  \n\nAccurate and timely detection of skin lesions, including melanoma and other types of skin cancer, is crucial for early intervention and improved patient outcomes.  \n\nTwo distinct approaches in AI support skin lesion diagnostics. The first one involves machine learning, 4 including neural networks and, especially, various types of convolutional neural networks (CNNs), 5 which require numerous and reliable learning data.4,5 These methods, however, often create black-box systems where reasoning is not understandable for human experts. The second approach uses expert systems that rely on information ontologies.  These systems employ understandable rules and dependencies, formalizing expert knowledge. Although expert systems offer transparency and interpretability, their preparation is labor-intensive.  \n\nThe integration of AI techniques, such as machine learning and expert systems, with dermatology has led to advancements in automated analysis of skin lesion images. These AI algorithms have demonstrated promising results in various tasks, including skin lesion classification, segmentation, feature extraction, and risk prediction. By leveraging large data sets of annotated skin lesion images, AI models are capable of learning complex patterns and discriminating between benign and malignant cases. The ability to extract relevant features from skin lesion images enables AI systems to provide valuable insights into lesion characteristics, aiding dermatologists in their own decision-making process.  \n\n# Methodology  \n\nThis contribution aims to present novel perspectives on skin lesion analysis using AI. Alongside established methods, we will explore emerging trends and innovative techniques. These new perspectives may encompass various aspects, such as the integration of multimodal data, the use of deep learning architectures, and the incorporation of advanced image-processing algorithms. By presenting them, we aim to inspire further research and to foster the development of more effective and reliable AI-based solutions for dermatologic applications.  \n\nIn the section on modern classification we have shown a few examples of recent work on the classification of skin lesions in dermatoscopic images. We have limited our brief review to documents published since 2019 that deal with the classification of melanocytic lesions. Additional limitations in the choice were the description of the method used in a way that allowed the experiment to be repeated and the research to be carried out for a wide set of data.  \n\n# Large data sets with annotated images of skin lesions  \n\nTo develop methods of image analysis based on AI, especially in the case of methods using deep convolutional networks, it is necessary to have significantly numerous (at least several thousand) sets of annotated training and test images. Although skin lesions are a common problem, individual scientific units usually have limited sets of annotated images of such lesions. Fortunately, there are several publicly available databases of annotated images with skin pigmentation lesions.  \n\nOne of the most popular large data sets used for training and testing methods for skin lesion classification is HAM10000.7.8 This data set contains dermatoscopic images from different populations, acquired and stored by different modalities. The data set consists of 10,015 images, which can serve as a training set for academic machine learning purposes. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions, including actinic keratoses and intraepithelial carcinoma/Bowen disease, basal cell carcinoma, benign keratosislike lesions (solar lentigines/seborrheic keratoses and lichen-planus like keratoses), dermatofibroma, melanoma, melanocytic nevi, and vascular lesions (angiomas, angiokeratomas, pyogenic granulomas, and hemorrhage).  \n\nThe images of the HAM10000 set were collected over a period of 20 years from two different sites: the Department of Dermatology at the Medical University of Vienna, Austria, and the skin cancer practice of Cliff Rosendahl in Queensland, Australia. Some of the images originate from the time before the era of digital cameras, so they were stored as diapositives and later converted to digital form. All images are stored as JPEG files with sizes of $800\\times600$ pixels. More than $50\\%$ of lesions have been confirmed by pathology; the diagnostic basis for the remaining cases was either followup, expert consensus, or confirmation by in vivo confocal microscopy.  \n\nThe other popular data sets come from a series of challenges hosted by the International Skin Imaging Collaboration (ISIC). 9 The ISIC is an international effort to improve melanoma diagnosis, sponsored by the International Society for Digital Imaging of the Skin. 10 Between 2016 and 2020, there were five such challenges, and each of them had its own set of data. Because this contribution concerns disease recognition, we will limit ourselves to data sets for disease classification, as in some challenges, tasks other than classification were considered.  \n\nThe first challenge was held in 2016. 11 The full name of the challenge is “Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the International Symposium on Biomedical Imaging (ISBI).” Its data set is referred to as ISCS2016 or ISBI2016. Lesion classification data included the original image accompanied by the definitive malignancy diagnosis and the lesion segmentation mask. This had three parts: lesion segmentation, detection and localization of visual dermoscopic features/patterns, and disease classification. The data set for the challenge has grown over time. Details of the data set for the classification part are given in Table 1.  \n\nFigure 1 shows examples of images from the ISIC 2019 challenge. 14  \n\nTable 1 Details of International Skin Imaging Collaboration challenge data sets   \n\n![](images/b624cefc0fbe23ccc4edf6cbea48bb146e397a0eee4840cb1a1f741e2d176591.jpg)  \n\n![](images/c4cd8c5739c0b8cbd553e3c50a84cd824e28075cad69c1d068f7d9d87db2e878.jpg)  \nFig. 1 Examples of images from the ISIC 2019 challenge.1 4 The upper row from the left: melanoma, melanocytic nevi, basal cell carcinoma, actinic keratoses. The bottom row from the left: benign keratosis-like lesions, dermatofibroma, vascular lesions, squamous cell carcinoma. ISIC, International Skin Imaging Collaboration.  \n\nSome less popular data sets are hosted by the DermNet service from New Zealand. 17 DermNet describes itself as “the world’s leading free dermatology resource . . . created for healthcare professionals to serve as a comprehensive database of skin conditions.” This data set contains slightly more than 20,000 images obtained by various techniques; however, there are only 191 images of melanoma. Free images are watermarked, and clean images can be downloaded for a fee. Dermofit 18 is a dermoscopic image library provided by the University of Edinburgh. It consists of 1,300 skin images containing 10 types of skin lesions, including 331 images of melanoma. These images can be downloaded under an academic license for a fee.  \n\n# Ontology-based expert systems  \n\nResearchers have increasingly focused their efforts on developing formal computational frameworks that express their expertise, knowledge, concepts, and relationships in a manner that is readable and applicable to decision-making expert systems in the medical field. In the domain of skin-related diseases, digital skin images exhibit diverse semantic features, presenting a considerable challenge in understanding symptoms and detecting early-stage skin cancers. To address this, the formalization and rendering of these standards in a machine-readable format through the use of ontologies becomes imperative.  \n\nOntologies serve as formal representations that define categories, properties, and relationships between concepts within specific subject areas. They play a crucial role in providing a precise and shared vocabulary for the semantic annotation of resources and describing them through metadata. In dermatology, biomedical ontologies provide essential domain knowledge that facilitates data integration, information retrieval, data annotation, natural language processing, and decision support. Considering the visual nature of dermatology and its substantial reliance on visual analysis, the annotation of images plays a crucial role in medical decision expert systems. This underscores the value of formalizing ontologies to build a knowledge base that facilitates inference and decision-making processes.  \n\nThe ABCD rule forms the foundation for dermatologists’ diagnoses, encompassing the Asymmetry, Border structure, variegated Color, and Differential structure characteristics of skin lesions. This rule is generally formulated, however, and diagnosing skin lesions requires a deeper understanding of subtle differences, which dermatologists have acquired through experience. Creating a strict, detailed, and precise description that enables accurate differentiation and classification of various skin lesions demands substantial work and the formalism provided by ontologies.  \n\nBiomedical ontologies play a vital role in providing essential domain knowledge for driving data integration, information retrieval, data annotation, natural language processing, and decision support. To enable inference over classes, description logic (DL) attributes, domain, and range constraints are used. The ontology for skin lesions 19 was implemented in OWL format, leveraging the capabilities of DL to establish a standardized syntax that eliminates semantic ambiguity and is suitable for a robust inference engine. A DL-based approach offers distinct advantages, especially in the interpretation of medical images, where extensive domain knowledge is involved.  \n\nThe proposed ontology model adopts a multilevel hierarchy, where a collection of graphical descriptors encompasses a set of image subjects, which, in turn, encompasses a set of medical terminology. The authors specifically focus on image attributes related to the ABCD principle, such as contour features, asymmetry, surface color, and texture features. The ontology serves as the foundation for developing an expert system that supports the annotation of melanocytic changes. The implemented program calculates image attributes and performs inference by considering the semantic relationships between the quantitative attributes of the images (data) and the quality (classes) of the changes.  \n\nThe presented results 19 demonstrate the precision and recall values obtained when distinguishing cases of melanoma, dysplastic changes, and common skin nevi. For melanoma, a precision of 0.75 and a reproducibility of approximately 0.65 were achieved. In the case of dysplastic changes, the precision exceeded $80\\%$ , and the reproducibility was more than 0.70. For common changes, the precision reached 0.80, with a reproducibility of more than 0.65.  \n\nThe authors of the contribution 20 introduce an ontology that specifically focuses on the classification of skin nevi and its semantic annotation. Drawing inspiration from dermatology experts, the Bag-of-Words (BoW) technique models these concepts in diagnosing skin lesions. The BoWs are constructed based on the features extracted from the lesion images, encompassing shape, color, and texture descriptors. A machine learning support vector classifier generates the BoWs from these features. The authors also derived their ontology from the scoring system of the ABCD rule. Experiments with the developed annotation system were conducted on a publicly available database comprising 206 lesion images, demonstrating that the ontology provided an efficient framework for analysis. Incorporating semantic relations between concepts facilitated the integration of expert knowledge, leading to a more appropriate approach for classifying lesion severity. A comparison was made between the results obtained using the proposed method and those achieved by other approaches. The presented approach demonstrated a high level of accuracy equal to $0.77\\%$ and a sensitivity of  \n\n0.97; however, the obtained specificity of 0.48 can be regarded as unsatisfactory.  \n\nThe issue of insufficient specificity was addressed by implementing fuzzy reasoning principles.21,22 In this regard, the developed ontology was enhanced by incorporating vague decision aspects. The recognition task, differentiating between the melanoma and benign classes, was performed using a $k$ -nearest neighbor classifier with a parameter $k$ set to 7. This approach yielded an accuracy of 0.92 and a sensitivity of 0.96. The specificity increased to 0.89, which is a promising outcome. To evaluate the performance of this approach, a comparison was made with results obtained from other algorithms, including two convolutional neural networks. The findings demonstrated that using the fuzzy decision approach and ontology-based inferencing is a viable alternative to these networks.  \n\n(Editor’s note: The aforementioned systems may be enhanced by using the ABCDE rule, in which the “E” stands for “evolution.” Applying this rule requires that the appearance be evaluated over time, preferably by taking good clinical and dermoscopic images at each observation. Changes in the appearance of the lesion, particularly expansion, are considered signs of malignancy.)  \n\n# Modern classification systems for dermatoscopic images of skin lesions  \n\nThe Global-Part Convolutional Neural Network (GPCNN) 23 model integrates fine-grained local and global context information. The Global-Part model comprises a Global Convolutional Neural Network (G-CNN) and a Part Convolutional Neural Network (P-CNN). The G-CNN interacts with downscaled dermoscopy images to extract information at the global scale and generate the Classification Activation Map (CAM). The P-CNN is trained with CAM-guided image patches to capture local-scale information in skin lesion regions. An ensemble strategy is employed to enhance the classification performance further. The ensemble combines discriminant information from GP-CNNs separately trained on the same image set.  \n\nIn another approach, the Geometric Active Contour $\\mathrm{(GAC)^{24}}$ was used to segment the area of lesions and isolate them from healthy skin. Subsequently, pretrained standard CNN models, AlexNet, GoogleNet, and VGG16, followed by global average pooling, are applied to generate features. Feature vectors are then reduced through principal component analysis (PCA) and, finally, fed into the artificial neural network (ANN) and random forest (RF) classifiers.  \n\nThe fusion of three deep learning architectures 25 — Xception, Inception-ResNet-V2, and NasNetLarge— pretrained on images from ImageNet is employed for the Large-Scale Visual Recognition Challenge 2012. 26 Subsequently, transfer learning is applied to all these architectures using data from the ISIC 2019 database. All layers except the last ones undergo transfer learning, while the output layer is trained from scratch. In the next step, a LightGBM framework, a version of Gradient Boosted Decision Tree (GBDT), is trained using the features extracted from the last pooling layer of each deep learning model. As in the previous step, score-level averaging is applied to combine the prediction scores assigned to each class for the augmented images from the test set, locally, within a single network, and globally among different models.  \n\nAnother approach uses a multistep algorithm. 27 The first step involves preprocessing, including contrast enhancement through fast local Laplacian filtering, followed by a transformation to the HSV (Hue, Saturation, Value) color space, which is performed. In the second step, segmentation into lesion and healthy skin areas is performed using a transferlearned VGG19 network operating on the images from the first step. Subsequently, the original image is masked by the segmentation outcome so that healthy skin pixels from the original image are set to zero. Finally, the Inception V3 network is employed to obtain the feature vector. These feature vectors are then reduced based on entropy, and a fully connected network is applied for classification into three classes.  \n\nThree popular CNNs, DenseNet-201, Inception-ResNetV2, and Inception-V3, 28 were trained on the ImageNet data set. 26 The authors tested seven different optimizers and four different numbers of dense layers. Two training sets were used: one with the original HAM10000 data set and the second with an augmented HAM10000 data set in which the numbers of samples in classes were balanced. Two training approaches were tested. In the first, only dense layers were trained from scratch, whereas in the second approach, selected convolutional layers were transfer-learned, and the dense layers were trained from scratch. The best results were reported for DenseNet 201 with transfer learning on an augmented balanced data set.  \n\nA modified GoogleNet 29 was used to classify images of skin lesions. 30 GoogleNet was pretrained on the ImageNet 26 data set. The modification of the GoogleNet architecture consisted of replacing the last three layers with a new fully connected layer with 2,048 neurons, followed by the output classification layer with the SoftMax activation function. This new network was transfer-trained on the ISIC 2019 data set. Improvement in the balance of training data was obtained in two ways: in the first, the minor numerous classes were augmented; in the second, the data for individual classes were randomly reduced, adjusting their number to the lowest numbered class.  \n\nThe DermoExpert 31 is a two-stage framework. The first step is preprocessing, which involves segmentation, rebalancing, and augmentation. The segmentation uses DSNet. 32 A weighted loss function is used for rebalancing, and augmentation relies on rotation and intensity changes. After the preprocessing, a so-called Hybrid CNN classifier is employed. This classifier comprises three custom-designed parallel CNN networks called feature generators. The outputs of the feature generators are fused through channel concatenation. The fused output and the separate outputs of the feature generator are processed by global average pooling layers and provided to four identical but separate dense networks with three fully connected layers, followed by the output layer with a SoftMax activation function. The outputs of these thick layers are averaged and provided as the final output. The weights of the feature generators were pretrained on ImageNet data. 26  \n\nThe best algorithms for recognizing skin lesions can be found at the top of the leaderboard from ICIS challenges, especially the latest one, SIIM-ISIC Melanoma Classification from 2020. 10 Unfortunately, the challenge with these algorithms is that they are rarely documented in scientific contributions. Occasionally, summaries or reports are available to describe the approach taken. On the positive side, the training data set (images plus classification) is extensive, publicly available, and thoroughly described on the challenge website. The test set was provided without classification, and the results were submitted to the challenge organizers for evaluation, ensuring objectivity. Many participants adopt an approach to publish the complete code used during the challenge and their report, allowing others to replicate the entire learning and testing process. The majority of participants at the top of the results ladder employ an ensemble of classifiers, with most or all of the classifiers being deep convolutional networks. This trend is evident even in the case of the winner 33 of the ISIC 2021 challenge, whose code is accessible on the open GitHub repository. 34  \n\nThere are also some problematic works; for example, in one contribution, 35 the authors used the HAM10000 data set. The images were resized to $28\\times28$ pixels, reducing some lesions to 20 pixels. To balance the classes, all classes except melanocytic nevi were augmented by flipping and random rotation in the range of $\\pm10^{\\circ}$ . This expanded data set was divided into 2/3 for the training set, 1/6 for the validation set, and 1/6 for the test set. A relatively simple CNN with five convolutional layers, one fully connected layer, and an output layer was used for classification. A max-pool layer followed each convolutional layer. The entire network had about 172,000 parameters. Reported results are excellent, with an average accuracy, precision, recall, and F1 score of 0.98.  \n\nA closer look at the classification parameters of individual classes indicated that the best results were achieved for the most heavily augmented classes. For example, for the class Dermatofibroma, which was supplemented more than 50 times, all reported classification measures were 1. The least increased class was basal cell carcinoma, supplemented more than seven times, and reported classification measures were between 0.94 and 1. In the case of the melanocytic nevi class, accuracy and reproducibility were at the level of 0.87, the F1 score was at the level of 0.93, and precision was at the level of 1. Considering this, after the proposed augmentation of images with such low resolution, they did not differ much from the original, and the network was tested on known images. Such a significant reduction in resolution is a conceptual error because some structures necessary for humans to recognize melanocytic lesions have been blurred here.  \n\nAn interesting study focused on the improvement in the diagnosis of skin lesions supported by analysis using CNNs. 36 In this research, the commercial FotoFinder 37 system was used. This system holds an MDR (Medical Device Risk) Class IIa certificate and CE (Conformité Européene) mark, making it approved for medical use in the European Union (EU). With its assistance, 188 patients from two medical units involving 22 dermatologists in the study were examined over two years. A total of 228 suspicious melanocytic lesions were detected in the examined patients, of which 38 were melanoma.  \n\nThe research demonstrated that diagnostic accuracy was significantly improved when CNN-based skin lesion analysis was incorporated into decision making. The mean sensitivity improved from $84.2\\%$ to $100\\%$ , the mean specificity improved from $72.1\\%$ to $83.7\\%$ , and the mean accuracy improved from $74.1\\%$ to $86.4\\%$ . Unfortunately, the authors did not provide details on the CNN network used, and the data set is limited.  \n\nTable 2 summarizes publications on skin lesion classification methods. Accuracy measures presented here are specifically for the detection of malignant melanoma. Many existing publications use average measures to classify various moles, which can often inflate these measures due to unbalanced data sets. We chose to focus on measurements related to melanoma detection because it poses the greatest threat to patients, unlike other skin lesions.  \n\nThe table excludes contributions in which we identified methodologic problems and those with inaccurate descriptions of the methods used, rendering the experiment impossible to reproduce. The last entry in the table pertains to the winner of the ISIC 2020 challenge. No scientific publication is available in this instance, but the authors have provided a brief report with the complete code accessible in an open repository.  \n\n# Smartphone applications  \n\nSmartphone devices are widely used and accessible portable computers with enriched connectivity options. Equipped with cameras and more computational power, they are becoming increasingly helpful in various medical applications. Some of them are assisting in skin selfexamination, tracking the evolution of suspicious lesions, or being used for storage and forward teledermatology. To increase the quality of the images, there are also specialized lens adapters—mobile dermatoscopes, such as DermLite 38 or MoleScope 39 —that can zoom in on suspicious lesions. These devices enable patients to monitor their skin lesions and moles from home and send high-quality images for regular checkups. They allow one to take photos with high magnification and use cross-polarized or polarized light. There are also dedicated applications and databases in which the doctor can store, compare, or analyze these images. 40-42  \n\nThe developed systems are often implemented as mobile applications to expand their availability and the ability to collect new data. Basic information on the most significant research is summarized in Table 3.  \n\nThe first attempt to implement an automatic classification system in mobile applications used a traditional approach for the parametric description of skin lesions. 43 Iowa State University researchers developed an application that extracts the ABCD features (Asymmetry, Border irregularity, Color variegation, and Diameter) from photographed lesions. The smartphone’s detachable $10~\\times$ lens is used to enhance the image quality. Before the feature extraction operation, image preprocessing and segmentation are performed. The extracted features are input data for the support vector machine (SVM) classifier. The authors tested three kernels for the classifier: linear, radial basis function (RBF), and polynomial against evaluation parameters. The training and testing of the system were performed using the publicly available database that consists of 200 dermatoscopic images (80 atypical nevi, 80 common nevi, and 40 melanomas). To balance the number of samples in the classes, the data augmentation using the Synthetic Minority Oversampling Technique (SMOTE) was performed. The best result of the system was achieved using an SVM classifier based on an RBF kernel: sensitivity of 0.8, specificity of 0.90, and accuracy of 0.88.  \n\nAnother smartphone application 47 uses a convolutional neural network (CNN). The network architecture evaluation was performed on a total of 8,000 images. The CNN classifier obtained an accuracy of 0.788, a sensitivity of 0.913, and a specificity of 0.73. This part of the presented work seems to add functionality to the developed application rather than its core value. The emphasis of the work has been placed on the standard approach of image processing, ABCD feature extraction, and its presentation in the augmented reality application that works in real time on the smartphone device.  \n\nThe mobile application can also act only as a front-end of the designed system. 48 The data processing and neural network calculations are performed on the remote server to which the smartphone client application connects. The primary role of the mobile app is to gather images of the skin lesions and provide simple clinical descriptions. After that, it sends them to the server to add them to the database or to get the result of processing and diagnosis. The clinical information added to each image of the lesion is its description with yes/no data referring to increased (over time) pain, changes, bleeding, and elevation of its surface. The PAD-UFES-20 data set with 658 cancers and 1,399 noncancer lesions, 2,057 in total, confirmed this. The CNN used was modified using ResNet50, as described in a contribution published in 2020. 49 The authors described in detail the approach for CNN training and testing, including CNN structure and parameters, evaluation criteria, impact of clinical information, and data balancing.  \n\nTable 2   Summary of specific classification parameters obtained for melanoma class   \n\n![](images/8a5397a39afa565c6eddacf6be9592cd66167dd1043ff4a23cc47426be0af0c2.jpg)  \nAI, artificial intelligence; CNN, convolutional neural network; ISIC, International Skin Imaging Collaboration.  \n\nTabte 3 Summary of methods used in a smartphone application for a skin examination   \n\n![](images/62ccce5a2e97c2355f5e1d8baccc5fcc898fddbdc29972e206df1692cdaff975.jpg)  \nI, artificial intelligence; CE, Conformite Europeene; CNN, convolutional neural network; MDR, medical device risk; RBF, radial basis function; SVM, support vector machine.  \n\nThe obtained results attained an accuracy of 0.85 and a reproducibility of 0.96. These values were the result of the approach where the clinical data were also taken into consideration as input data without the images. The improvement varies between evaluation parameters but falls between 1.4 and $2.4\\mathrm{pp}$ .  \n\nAnother approach used a pretrained CNN-type MobileNet network. 50 The fine-tuning training was performed using the HAM10000 database (10,000 images). It contains the images of seven classes of pigmented lesions (not only cancers), including dermatofibromas, vascular lesions, actinic keratoses, basal cell carcinomas, benign keratosis-like lesions, melanomas, and melanocytic nevi. Finally, more than 6,000 images were divided into training, validation, and testing subsets in the ratio of $60\\%/20\\%/20\\%$ , respectively. CNN was implemented and trained in a TensorFlow library with a Keras package. To implement the trained network into an Android application, it had to be converted to a TensorFlowLite library supported in an Android Studio IDE (Integrated Development Environment). It was claimed that the application achieved an accuracy of 0.85; however, the authors present neither a detailed specification of testing and accuracy calculation nor a training description. The CNN accuracy obtained with the use of smartphones was lower than with the use of computers. The authors concluded that this is a consequence of converting the trained model to be compatible with the lightweight version of the TensorFlow library.  \n\nOne of the newer publications presents a user-friendly app for easy and quick cancer risk assessment of skin lesions. 51 The patients under examination by the system were also assessed by two dermatologists or with the histology of the lesion, if available. These authors proposed a two-stage approach using a different neural network for each. The process was divided into image classifier (analysis) and image region proposal (detection). The analysis stage was based on CNN, while the detection of novel stratification CNN was developed on a region proposal network (RPN). The second one, more computationally demanding, takes around two to five seconds for processing on modern smartphones. A threelevel decision for assessing images was introduced: subcategory, category, and risk level. Different vendors and models of smartphones were used for processing and acquisition of skin images. During this research, 238 patients with 1,171 lesions were examined, resulting in 18,384 images. The detected RPN model achieved a sensitivity and specificity of $96.4\\%$ and $94.85\\%$ , whereas CNN analysis yielded $0.954\\%$ and 0.903, respectively. The final application has been commercialized under SkinScreener; it has an MDR Class IIa medical device certification and complies with the strictest EU data protection requirements.  \n\nLast but not least is the SkinVision app, the most cited55354 and reviewed smartphone application.42435 SkinVision is a clinically validated, regulated medical device. It has an MDR Class IIa certificate, CE mark, TGA (Therapeutic Goods Administration) approval, and ISO 13485:2016 certification. SkinVision sponsored a few clinical studies on the accuracy of the SkinVision app’s risk assessment for University Hospital Munich, Catharina Hospital Eindhoven, clinical studies, and SkinVision app user databases. 54 This app was developed in 2011 and is still commercially available. The implemented algorithms were evaluated and improved for better sensitivity during this period. Since 2018, they have replaced the rule-based classification algorithm with a machine learning approach (Udrea et al, unpublished material, 2019). It uses a machine learning algorithm to analyze spots on the skin. A smartphone application’s accuracy for triaging skin lesions based on machine learning algorithms has also been evaluated. 45 These results have demonstrated a sensitivity of 0.95 and a specificity of 0.78. More than 130,000 images were used for training. The Conditional Adversarial Network algorithms combined with an SVM classifier were used to segment the lesions.  \n\n# Total body systems  \n\nTotal body (or full-body, whole-body) systems (further referred to as TBS) represent an alternative for skin lesion analysis, including melanoma detection. Contrary to classic approaches, in which the dermatologist focuses on a given lesion characterized by a single image acquired by a dermatoscope, high-quality camera, or smartphone, TBS captures a series of photographs covering the patient’s entire body. In this way, one can detect suspicious lesions and find lesions that have appeared since the last examination. Such followup examinations also make it possible to see lesions whose size has increased, which may suggest neoplastic changes. TBS are not commonly described in the literature, especially those that use AI-based algorithms.  \n\nOne of the oldest systems was designed for lesion detection in optical images of larger body fragments 56 (arm, forearm). Image analysis has included skin detection, hair removal, and lesion detection based on difference-of-Gaussian filters, followed by the SVM classifier. The obtained results show sensitivity in the range of 0.80 to 0.85 and accuracy in the range of 0.76 to 0.79, depending on SVM working conditions.  \n\nA full-body system 57 was created to discriminate new and rapidly evolving melanocytic lesions and to implement a three-dimensional (3D) stereo capture system with 22 acquisition units. Further image preprocessing includes the removal of artifacts, particularly hair that might be visible in some photos. Next, Laplacian-over-Gaussian filtering was applied for lesion segmentation. The linear discriminant analysis was used for lesion classification—the system identifies “suspect” lesions that can be new and modified. The lesions were analyzed for different size groups (up to 3, 5, and $7\\mathrm{mm}$ ). The resulting recall for groups was 0.9, and precision varied from 0.5 for small new lesions to 0.8 for larger ones. The main limitations of this study were the use of artificial lesions for algorithm testing and system evaluation performed for a small number of patients (six men and six women).  \n\nAnother TBS is designed to detect size change in multiple pigmented skin lesions using generated mole maps. 58 The system is based on a cabin equipped with 21 high-resolution cameras that capture the patient’s body in 24 positions. The lesions are detected using the MSER (maximally stable extremal regions) algorithm. A set of complex algorithms, that is, lesion geometry, is applied for lesion matching and assessing its size change. The system performance was summarized by stating that the system detects almost $100\\%$ of the lesions and was tested on several patients. The system matching algorithm has subsequently improved by implementing rigid transformations of 3D point clouds and using non-rigid coordinate plane deformations in local ROIs (Region of Interest) around the lesions of interest. 59 Matching algorithms were tested in the melanoma unit of the Hospital Clínic de Barcelona Barcelona, Spain, on many thousands of lesions, reaching a recall of 0.82 and a precision of 0.999.  \n\nOne contribution 60 presents a broad body image analysis to detect suspicious pigmented lesions. More extensive lesions (diameter ${>}3\\mathrm{mm}$ ) were manually segmented, and then ABCD features were calculated for each lesion in the image. Logistic regression was used for lesion classification, producing 0.84 sensitivity and 0.75 accuracy in detecting suspicious lesions. The system was tested on 133 patients and, altogether, 1,759 skin lesions.  \n\nA deep learning approach to detecting and classifying suspicious lesions in total body images was also implemented. 61 The lesions were identified using a blob detection algorithm using Laplacian-of-Gaussians filtering and scale-invariant feature transformation. Different deep network architectures (CNN, VGG16, Xception with a transfer learning model) were implemented for identified lesion classifications. The six classes include nonsuspicious lesion types A and B (defined in the text), suspicious lesions, skin, skin edges, and background. The system was tested on many lesions (more than 33,000), reaching 0.80 average accuracy for lesions in the suspicious classification endpoint. The developed CNN algorithm also obtained good agreement with dermatologists’ assessment.  \n\nAnother prototype of the TBS system was developed by Skopia Esthetic Clinic, Kraków, Poland, in cooperation with the Institute of Electronics, Lodz University of Technology, Poland. 62 To acquire the patient’s pictures, the system is equipped with digital cameras that take 32 overlapping pictures that cover almost the whole area of the patient’s skin, as shown in Figure 2. The prototype is equipped with image processing software that enables, among others, lesion detection and segmentation algorithms that use the Yolo deep network and an Otsu algorithm. The system also implements a procedure for building a 3D patient model. This permits acquired skin images with the model and orthorectify them for detecting size and shape changes in nevi. 63 It is also possible to see new lesions (or growing lesions) in followup studies. The lesion pairing algorithm implements the feature matching method (features are generated by scale-invariant feature transformation, BRISK, and AKAZE algorithms), and wrong matches are removed by the RANSAC algorithm. 64 The matching accuracy is 0.9, and the precision of lesion detection is 0.7 to 0.91, depending on lesion size. It is possible to detect changes in the size of moles of $1\\mathrm{mm}$ .  \n\n![](images/ffd9680f9eebd871a36e2b8d4b9b5c4120367574c40915052e5e0e5ab584948f.jpg)  \nFig. 2 Example of device used for capturing pictures of the total area of human skin.6 5  \n\nIn the literature, AI-based algorithms are also reported as convolutional neural networks 66 for analysis of the wholebody images to detect skin spots and classify them as nevi or non-nevi. The algorithm was tested on images from the Mind Your Moles study. 67 Images of 82 individuals (57,742 lesions) were considered in the training set, and data from 10 individuals (4,868 lesions) comprised the test set. The reported system performance of naevi detection (compared to senior dermatologists’ assessment) shows sensitivity and specificity of 0.79 and 0.91, respectively, for lesions ${\\geq}2\\mathrm{mm}$ and 0.84 and 0.91 for lesions $\\ge5\\mathrm{mm}$ . Another such algorithm is described in another contribution. 68 It detects and longitudinally tracks skin lesions on 3D total-body skin surface scans from the 3DBodytex database. The algorithm was trained using Faster R-CNN. The obtained results show similar lesion detection accuracy when compared to three human annotators. The achieved accuracy for the matching algorithm was 0.88, and the accuracy of lesion detection was 0.71.  \n\nThe performance of such systems is presented in Table 4. None of the discussed TBS is medically certified. The exception may be the ATBM master system (FotoFinder, Germany, https://www.fotofinder.de/en/technology/ total- body- dermoscopy/bodystudio- atbm/master), which allows for the early detection of skin changes. AI algorithms have been implemented in this system, but the manufacturer has not specified what kind of image analysis they perform, nor has the system performance been specified.  \n\nTable 4   Performance of total body systems   \n\n![](images/aeba4955479f151453f44fb6b643532e16f425f32e541a7168b68b35370b0c6e.jpg)  \n\n![](images/75031917808e6b9ddf00d8b9e25b2a12f803fdda97dc5cfc49e8d87ef040d5ac.jpg)  \nFig. 3 The number of contributions indexed by PubMed,7 1 Scopus,7 2 and Web of Science7 3 using the query “skin lesion classification CNN.” Data as of July 1, 2023 (not normalized).  \n\nSome other TBS are described in a review paper. 70 The vast majority of them do not implement AI-based approaches. This study summarizes the current evidence on TBS for the early detection of melanoma. The systems analysis was conducted for clinical applications, including melanoma incidence, biopsy rates, and Breslow’s detected tumor index.  \n\n# Discussion  \n\nUsing ontologies and expert systems offers several advantages over traditional machine learning approaches. Whereas machine learning methods rely heavily on large data sets of verified images and training algorithms, ontologies provide a knowledge-driven approach incorporating expert insights and domain-specific information. Ontology-based expert systems support semantic annotation and inference capabilities to aid in the classification of lesion severity and the identification of potential skin cancers with high accuracy, with correct predictions exceeding $90\\%$ . By integrating expert knowledge and leveraging the semantic relationships defined in ontologies, expert systems can assist in decisionmaking processes, providing valuable insights for medical practitioners; however, due to the intensive development of AI methods using machine learning, in particular deep neural networks, expert systems constitute only a significant minority of the various skin analysis methods described in the literature.  \n\nNumerous examples of the classification of images depicting pathologic skin changes are found in the literature. A query for “skin lesion classification CNN” in the $\\mathrm{PubMed^{71}}$ database returned 105 indexed contributions. The same query in Web of Science 72 yielded 326 contributions, and in Scopus, 72 there are 433. The increasing number of publications in all these databases indicates that interest in this research topic is growing yearly (Figure 3).  \n\nUnfortunately, some contributions relate to research conducted on relatively small local image data sets. 74-76 Others involve experiments conducted on large data sets, but the details of the methods used are either not disclosed or are presented in a way making it impossible to repeat the experiment. 77-81 In other reports, serious methodologic errors have been made, such as excessive augmentation of selected classes followed by the division of images into training, 35 validation, and test sets. As a result, there may be similar images in each set, leading to overestimating the quantitative measures of classification accuracy. 35  \n\n# Interesting publications  \n\nThe most exciting publications are those in which the research was carried out on large data sets, and the image analysis method is presented in a way that allows the experiment to be repeated. The most objective skin lesion diagnosis results are available on ISIC challenge leaderboards, the most recent of which was published in 2020. 10 Although the methods used have rarely been fully published in scientific journals. Summaries or short reports describing the methods used are available. Additionally, the full details of the procedures used in the training and test phases are often published together. The most effective techniques for diagnosing skin lesions employ an ensemble of classifiers based on deep convolutional networks.  \n\nSmartphone applications is promising in the early-stage diagnosis of skin pathologies. The introduction of AI into smartphone applications has increased the ability to search, track, and diagnose skin lesions. It is essential to detect and diagnose lesions early to achieve a high probability of stopping the development of pathologic changes.  \n\nThe main advantages of the smartphone system approach include that the application can be widely distributed among medical personnel and patients and that the image database can be easily expanded, which is essential because deep learning artificial neural networks require a large amount of data to train efficiently. Such methods can be easily tested using cameras in smartphones.  \n\nThere are also many more critical reviews concerning such solutions.5,82-86 These contributions draw attention to the numerous limitations of the applied methods, including the poor quality of the images taken by smartphones, errors due to photographs being taken in a different light, lack of standardization in taking photographs when less experienced people do this using their smartphones, bias in skin cancer detection by AI due to the data set it was trained on, and decreased visibility of lesions on darker skin.  \n\nThe use of mobile dermatoscopes may increase their diagnostic value. One review 55 does not recommend the use of smartphone applications for skin lesion diagnosis due to the poor and variable performance of algorithms based on smartphones. The authors stated that these apps have not yet shown sufficient promise to warrant recommendation of their use. The current CE marking assessment processes are inadequate to protect patients against the risks created by smartphone diagnostic or risk-stratification apps.  \n\nAll the aforementioned apps have a disclaimer that they cannot replace a dermatologic examination. They cannot provide the diagnosis and are not a substitute for regular dermatologist visits to date.  \n\nTotal body systems may quickly and accurately analyze the patient’s whole body to detect suspicious lesions. Due to a rapid examination, the system may be implemented for screening or epidemiologic studies, but TBS also have some limitations87: $2\\%$ to $8\\%$ of melanomas are hypopigmented, and detection on a total body imaging system may be less reliable than with pigmented lesions. Also, AI systems are much less helpful in diagnosing rarer but necessary cutaneous and mucous membrane cancers such as trabecular carcinomas (Merkel cell cancers).  \n\nTotal body systems face significant challenges related to the need to detect lesions in images covering a large part of the patient’s body and matching pairs of lesions in followup examinations to detect new lesions or those that have changed or grown since the last examination. The relatively small number of systems60.61 also compromises the detection of neoplastic lesions. Automatic analysis of images from TBS systems is more complex than evaluating images of individual moles obtained from a dermatoscope or a digital camera. This is due to the small size of the imaged lesions and geometric distortions caused by the complex shapes of the human body, which results in not all moles being perpendicular to the camera axis. The AI algorithms used in such systems provide a mole detection sensitivity of 0.9 and a matching accuracy of 0.8. These values are promising; however, the methods described in the literature have been validated on a relatively small number of patients (several dozen people, several tens of thousands of analyzed nevi). These systems (except the system described in the study59) have also not been clinically validated.  \n\n# Diagnosis of skin lesions  \n\nThis review of various types of systems supporting the diagnosis of skin lesions shows that the AI methods are practical tools. Of particular note are the different deep network architectures that ensure the exact operation of such systems. This is confirmed by the obtained values of measures evaluating the effectiveness of the applied algorithms. Many of these algorithms have been validated on large data sets, increasing the credibility of their results. The implementation of AI may also reduce the costs of melanoma screening. As was demonstrated in 1988, the cost of diabetic retinopathy screening can be lower if AI algorithms are used instead of expert judgment. 88 A similar situation may occur in dermatology, in which screening tests require many imaging analyses performed by dermatologists or other health providers. Using AI in dermatology, particularly in image-based dermatologic diagnosis, may bring modern solutions and benefits to patients and improve diagnosing by making it more objective and reliable.  \n\nArtificial intelligence methods pose limitations and threats that apply to dermatology and other fields of medicine. One such limitation of using AI in therapy stems from challenges such as small and inadequately described training data sets, leading to limited accuracy and practicality in clinical applications. Relevant data and large, welldescribed image data sets are necessary for practical AI algorithm training; otherwise, AI algorithms may not obtain sufficient generalization, which will translate into wrong decisions (eg, when analyzing images from different medical centers). There is a risk of misdiagnosis or inaccurate results, especially when attempting to diagnose diseases with minimally available training examples. Image artifacts and a lack of unified disease identification criteria hinder the recognition and diagnosis of AI in dermatology. Addressing these limitations—including the shortage of rare disease cases of patients with rare skin cancer-prone genodermatoses, such as albinism and xeroderma pigmentosum, and patients who are cancer-prone due to treatment, such as solid organ transplant recipients—remains a significant challenge for implementing AI in dermatology. 89 Another limitation may be the incorrect methodology for using AI algorithms (eg, excessive data augmentation, incorrect validation of algorithms). There is also a risk that undue reliance on AI may lead to the decline of traditional diagnostic and treatment methods among dermatologists, especially the younger and less experienced. In the field of dermatology, future AI trends include increasing support for diagnostic platforms, the emergence of intelligent medical devices and instruments, the utilization of transmedial intelligent medical equipment, integration with 5G networks, the implementation of cloud-based intelligent health care models, and the popularization of AI consultations90; however, AI is not expected to replace dermatologists, as it lacks certain human qualities completely, and physicians will continue to play a vital role in patient care. Proper understanding and utilization of AI can provide convenience to physicians and improve the quality of services for patients in dermatology.  \n\n# Legal issues  \n\nThe use of AI in medicine also faces possible legal issues. The EU has legislative and policy initiatives relating to using such algorithms and recognizing them as typical clinical procedures. 91 Currently, a new AI liability legal framework is in development. 92 The United States seems to be more advanced in this regard, where the Food and Drug Administration (FDA), in collaboration with the International Medical Device Regulators Forum, has developed a certification path for medical devices implementing AI algorithms and the algorithms themselves. 93 Accepted solutions in the software category as a medical device can be used in clinical practice. Currently, the FDA has approved more than 500 such algorithms, 94 but none are suitable for dermatology (the same applies to the EU-supported algorithms). Currently, solutions intended for applications mainly in radiology and cardiology dominate, and to a lesser extent in neurology, gastroenterology and hematology. Appropriate legal regulations are also needed to enable patient medical data exchange between clinical centers and other scientific institutions to develop high-quality AI algorithms. Another aspect that should be considered when using AI is ethical issues, such as possibly informing and obtaining informed consent from patients for this diagnostic procedure. With further development of AI methods, as the level of their “awareness” and “decisiveness” increases, a vital issue to be resolved will be the issue of legal liability for any diagnostic errors made by the algorithms.  \n\n# Conclusions  \n\nThe contribution and role of AI in supporting dermatologic diagnostics are growing; a similar conclusion applies to other medical specialties. Developing these algorithms allows us to obtain results comparable to the assessment of specialists. In this context, AI emerges as a valuable tool that aids physicians, provided the algorithms are adequately learned and reliably validated.  \n\nThe effective integration of AI in diagnostics demands physicians to possess a suitable level of awareness and knowledge, often at the intersection of medicine, biomedical engineering, and informatics. They need to understand the limitations of the algorithms in use and the rationale behind diagnostic decisions facilitated by explainable AI. Consequently, clinical engineers are increasingly responsible for maintaining and developing these algorithms, including scenarios in which updates and retraining of neural networks are necessary.  \n\nMany issues about using AI in clinical practice remain to be resolved. These issues are technical (ie, ensuring access to a large amount of medical data of appropriate quality, which is necessary for the practical training of most of the algorithms used) and legal (ie, developing consistent rules and conditions enabling AI algorithms in the clinical setting).  \n\n# Declaration of competing interest  \n\nThe authors declare no conflicts of interest.  \n\n# References  \n\n2000;25:459-463.   \n2. Matthews NH, Li W-Q, Qureshi AA, Weinstock MA, Cho E. Epidemi  \n3. Tsao H, Olazagasti JM, Cordoro KM, et al. Early detection   \n4. Gowthami V, Sneha G. Melanoma detection using recurrent neural network. Lecture Notes Electr Eng. 2021;700:1563-1573.   \n5. Zhang B, Wang Z, Gao J, et al. Short-term lesion change detection for melanoma screening with novel Siamese neural network. IEEE Trans   \n6. Abu-Nasser B. Medical expert systems survey. Int J Soft Comp Math   \n7. Tschandl P, Rinner C, Apalla Z, et al. Human–computer collaboration   \n8. Tschandl P, Rosendahl C, Kittler H. The HAM10000 dataset, a large   \n9. International Skin Imaging Collaboration. Available at: https://www. isic-archive.com/. Accessed June 24, 2023.   \n10. Zawacki A, Helba B, Shih G, et al. SIIM-ISIC melanoma classification. Available at: https://www.kaggle.com/c/siim-isic-melanomaclassification/overview. Accessed July 1, 2023.   \n11. Gutman D, F Codella NC, Celebi E, et al. Skin lesion analysis toward melanoma detection: a challenge at the International Symposium on Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration (ISIC). arXiv. 2016:1605.01397.   \n12. Codella NCF, Gutman D, Celebi ME, et al. Skin lesion analysis toward melanoma detection: a challenge at the 2017 International symposium   \n13. Codella N, Rotemberg V, Tschandl P, et al. Skin lesion analysis toward melanoma detection 2018: a challenge hosted by the International Skin Imaging Collaboration (ISIC). arXiv. 2019:1902.03368.   \n14. Combalia M, Codella NCF, Rotemberg V, et al. BCN20000: dermoscopic lesions in the wild. arXiv. 2019:1908.02288.   \n15. Rotemberg V, Kurtansky N, Betz-Stablein B, et al. A patient-centric   \n16. International Skin Imaging Collaboration (ISIC). The ISIC 2020 Challenge Dataset. Available at: https://challenge2020.isic-archive.com/. Accessed June 28, 2023.   \n17. DermNet. Dermatology resource. Available at: https://dermnetnz.org/. Accessed July 8, 2023.   \n18. University of Edinburgh. Dermofit Image Library. Available at: https: //licensing.edinburgh-innovations.ed.ac.uk/product/dermofit-imagelibrary. Accessed July 8, 2023.   \n19. Maragoudakis M, Maglogiannis I. A medical ontology for intelli  \n20. Abbes W, Sellami D. Automatic skin lesions classification using ontol  \n21. Laskaris N, Ballerini L, Fisher RB, Aldridge B, Rees J. Fuzzy descrip  \n22. Abbes W, Sellami D, Marc-Zwecker S, Zanni-Merk C. Fuzzy decision ontology for melanoma diagnosis using KNN classifier. Multimed Tools   \n23. Tang P, Liang Q, Yan X, Xiang S, Zhang D. GP-CNN-DTEL: Global– Part CNN model with data-transformed ensemble learning for skin le  \n25. Ahmed SAA, Yanikoglu B, Goksu O, Aptoula E. Skin lesion classification with deep CNN ensembles. 2020 28th Signal Processing and Com  \n26. Russakovsky O, Deng J, Su H, et al. ImageNet large scale visual recog  \n27. Saba T, Khan MA, Rehman A, Marie-Sainte SL. Region extraction and classification of skin cancer: a heterogeneous framework of deep CNN   \n28. Villa-Pulgarin JP, Ruales-Torres AA, Arias-Garzón D, et al. Optimized convolutional neural network models for skin lesion classifica  \n29. Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions. Com  \n30. Kassem MA, Hosny KM, Fouad MM. Skin lesions classification into eight classes for ISIC 2019 using deep convolutional neural network   \n31. Hasan MK, Elahi MTE, Alam MA, Jawad MT, Martí R. DermoExpert: skin lesion classification using a hybrid convolutional neural network   \n32. Hasan MK, Dahal L, Samarakoon PN, Tushar FI, Martí R. DSNet:   \n33. Ha Q, Liu B, Liu F. Identifying melanoma images using EfficientNet ensemble: winning solution to the SIIM-ISIC melanoma classification challenge. arXiv. 2020:1908.02288.   \n34. Ha Q, Liu B, Liu F. haqishen/SIIM-ISIC Melanoma Classification 1st Place Solution. Available at: https://github.com/haqishen/ Accessed July 1, 2023.   \n35. Aldhyani THH, Verma A, Al-Adhaileh MH, et al. Multi-class skin lesion classification using a lightweight dynamic kernel deep-learn  \n36. Winkler JK, Blum A, Kommoss K, et al. Assessment of diagnostic performance of dermatologists cooperating with a convolutional neural net  \n37. FotoFinder. AI Moleanalyzer Pro. Available at: https://www.fotofinder. July 8, 2023.   \n38. DermLite. DermLite adapters for iPhone, any Android smartphone, iPad, or tablet. Available at: https://dermlite.com/collections/ connection-kits. Accessed June 29, 2023.   \n39. IDCP Molescope. Available at: htp://www.icp.emolecope-eu. Accessed June 29, 2023.   \n40. Alves J, Moreira D, Alves P, Rosado L, João M, Vasconcelos M. Automatic focus assessment on dermoscopic images acquired with smart  \n41. Yilmaz A, Gencoglan G, Varol R, Demircali AA, Keshavarz M, Uvet H. MobileSkin: classification of skin lesion images acquired using mobile   \n42. Jaworek-Korjakowska J, Kleczek P. ESkin: study on the smartphone application for early detection of malignant melanoma. Wirel Commun   \n43. Kalwa U, Legner C, Kong T, Pandey S. Skin cancer diagnostics with an   \n44. Mendonca T, Ferreira PM, Marques JS, Marcal ARS, Rozeira J. PH2: a dermoscopic image database for research and benchmarking. Proceedings of the Annual International Conference of the IEEE Engineer  \n45. Udrea A, Mitra GD, Costea D, et al. Accuracy of a smartphone application for triage of skin lesions based on machine learning algorithms.   \n46. SkinVision. Skin cancer melanoma detection app. Available at: https: //www.skinvision.com/. Accessed July 8, 2023.   \n47. Francese R, Frasca M, Risi M, Tortora G. A mobile augmented reality application for supporting real-time skin lesion analysis based on deep   \n48. Krohling B, Castro PBC, Pacheco AGC, Krohling RA. A smartphone based application for skin cancer classification using deep learning with clinical images and lesion information. arXiv. 2021:2104.14353.   \n49. Pacheco AGC, Krohling RA. The impact of patient clinical in  \n50. Maduranga P, Nandasena D, Maduranga M. Mobile-based skin disease diagnosis system using convolutional neural networks (CNN). IJ Image   \n51. Kränke T, Tripolt-Droschl K, Röd L, Hofmann-Wellenhof R, Koppitz M, Tripolt M. New AI-algorithms on smartphones to detect   \n52. SkinScreener. An app to assess the risk of skin lesions. Available at: http://skinscrener.com/en. Accessed July 8, 2023.   \n53. Thissen M, Udrea A, Hacking M, Von Braunmuehl T, Ruzicka T. MHealth app for risk assessment of pigmented and nonpigmented skin lesions: a study on sensitivity and specificity in detecting malignancy. Telemed $e$   \n54. de Carvalho TM, Noels E, Wakkee M, Udrea A, Nijsten T. Development of smartphone apps for skin cancer risk assessment: progress and   \n55. Freeman K, Dinnes J, Chuchu N, et al. Algorithm based smartphone apps to assess risk of skin cancer in adults: systematic review of diag  \n56. Taeg SC, Freeman WT, Tsao H. A reliable skin mole localization   \n57. Bogo F, Romero J, Peserico E, Black MJ. Automated detection of new or evolving melanocytic lesions using a 3D body model. Lecture Notes   \n58. Korotkov K, Quintana J, Puig S, Malvehy J, Garcia R. A new total body scanning system for automatic change detection in multiple pigmented orotk K, J, mp al.An impro skin lesion matching scheme in total body photography. IEEE J Biomed Health In  \n60. Birkenfeld JS, Tucker-Schwartz JM, Soenksen LR, Avilés-Izquierdo JA, Marti-Fuster B. Computer-aided classification of suspicious pigmented lesions using wide-field images. Comput   \n61. Soenksen LR, Kassis T, Conover ST, et al. Using deep learning for dermatologist-level detection of suspicious pigmented skin lesions from cation in full body imaging systems. In: Pietka E, Badura P, Kawa J,   \n65. Skopia Estetic Clinic. Dermo Studio. Available at: https://dermo-studio. eu/#kontakt. Accessed July 1, 2023.   \n66. Betz-Stablein B, D’Alessandro B, Koh U, et al. Reproducible naevus counts using 3D total body photography and convolutional neural net  \n67. Koh U, Janda M, Aitken JF, et al. Mind Your Moles” study: proto  \n68. Zhao M, Kawahara J, Abhishek K, Shamanian S, Hamarneh G. Skin3D: detection and longitudinal tracking of pigmented skin le  \n70. Hornung A, Steeb T, Wessely A, et al. The value of total body photography for the early detection of melanoma: a systematic review. Int J   \n71. PubMed. National Library of Medicine. Available at: https://pubmed. ncbi.nlm.nih.gov/. Accessed July 8, 2023.   \n72. Scopus. Elsevier B.V. Available at: https://www.scopus.com/search/ form.uri?display=basic#basic. Accessed July 8, 2023.   \n73. Web of Science Core Collection. Clarivate. Available at: https://www. webofscience.com/wos/woscc/basic-search. Accessed July 8, 2023.   \n74. Pramanik R, Banerjee B, Efimenko G, Kaplun D, Sarkar R. Monkeypox   \n75. Goceri E, Karakas AA. Comparative evaluations of CNN based networks for skin lesion classification. The 14th International Conference on Computer Graphics, Visualization, Computer Vision, And Image   \n76. Lihacova I, Bondarenko A, Chizhov Y, et al. Multi-class CNN for classification of multispectral and autofluorescence skin lesion clinical im  \n77. Nersisson R, Iyer TJ, Joseph Raj AN, Rajangam V. A dermoscopic skin lesion classification technique using YOLO-CNN and traditional fea  \n78. Alfaro E, Fonseca XB, Albornoz EM, Martinez CE, Ramrez SC. A brief analysis of U-Net and Mask R-CNN for skin lesion segmentation.   \n79. Dhar P, Guha S. Engineering and manufacturing, skin lesion detection ing on web: skin lesion classification using CNN. 2022 International   \n81. Sankar Raja Sekhar K, Ranga Babu T, Prathibha G, Vijay K, Chiau   \n82. Beltrami EJ, Brown AC, Salmon PJM, Leffell DJ, Ko JM, Grant-Kels JM. Artificial intelligence in the detection of skin cancer. J   \n83. Matin RN, Dinnes J. AI-based smartphone apps for risk assessment of   \n84. Ouellette S, Rao BK. Usefulness of smartphones in dermatology: a   \n85. Rat C, Hild S, Sérandour JR, et al. Use of smartphones for early detec  \n86. Xiong M, Pfau J, Young AT, Wei ML. Artificial intelligence in teleder  \n87. Rayner JE, Laino AM, Nufer KL, et al. Clinical perspective of 3D total body photography for early detection and screening of melanoma. Front   \n88. Grzybowski A, Singhanetr P, Nanegrungsunk O, Ruamviboonsuk P.   \n89. Steele L, Velazquez-Pimentel D, Thomas BR. Do AI models recognise rare, aggressive skin cancers? An assessment of a direct-to-consumer application in the diagnosis of Merkel cell carcinoma and ame  \n90. LiZ, Koban KC, Schenck TL, Giunta RE, Li Q, Sun Y. Artificial intelligence in dermatology image analysis: current developments and future   \n91. Artificial Intelligence and Medical Devices Regulation. Discussing the legal framework and the ethical challenges within the CORE-MD project. Available at: https://www.core-md.eu/artificial-intelligence2023.   \n92. European Commission. Liability rules for artificial intelligence. Availintelligence_en. Accessed July 8, 2023.   \n93. Food and Drug Administration. Global approach to software as a medical device. Available at: https://www.fda.gov/medical-devices/ device. Accessed July 3, 2023.   \n94. Food and Drug Administration. Artificial intelligence and machine learning (AI/ML)-enabled medical devices. Available at: https://www. fda.gov/medical-devices/software-medical-device-samd/artificialAccessed July 3, 2023.  "}