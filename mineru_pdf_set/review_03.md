{"md_content":"# A Comparison of Full and Partial Predicated Execution Support for ILP Processors  \n\nScott A. Mahlke $^*$ Richard E. Hank James E. McCormick David I. August Wen-mei W. Hwu  \n\nCenter for Reliable and High-Performance Computing University of Illinois Urbana-Champaign, IL \u0006\u0001\b0\u0001  \n\n# Abstract  \n\nOne can e\u000bectively utilize predicated execution to improve branch handling in instruction-level parallel processors. Although the potential bene\fts of predicated execution are high, the tradeo\u000bs involved in the design of an instruction set to support predicated execution can be di\u000ecult. On one end of the design spectrum, architectural support for full predicated execution requires increasing the number of source operands for all instructions. Full predicate support provides for the most \rexibility and the largest potential performance improvements. On the other end, partial predicated execution support, such as conditional moves, requires very little change to existing architectures. This paper presents a preliminary study to qualitatively and quantitatively address the bene\ft of full and partial predicated execution support. With our current compiler technology, we show that the compiler can use both partial and full predication to achieve speedup in large control-intensive programs. Some details of the code generation techniques are shown to provide insight into the bene\ft of going from partial to full predication. Preliminary experimental results are very encouraging: partial predication provides an average of $33\\%$ performance improvement for an \b-issue processor with no predicate support while full predication provides an additional $30\\%$ improvement.  \n\n# \u0001 Introduction  \n\nBranch instructions are recognized as a major impediment to exploiting instruction-level parallelism (ILP). ILP is limited by branches in two principle ways. First, branches impose control dependences which restrict the number of independent instructions available each cycle. Branch prediction in conjunction with speculative execution is typically utilized by the compiler and/or hardware to remove control dependences and expose ILP in superscalar and VLIW processors [\u0001] [\u0002] [\u0003]. However, misprediction of these branches can result in severe performance penalties. Recent studies have reported a performance reduction of two to more than ten when realistic instead of perfect branch prediction is utilized [\u0004] [\u0005] [\u0006]. The second limitation is that processor resources to handle branches are often restricted. As a result, for control intensive applications, an arti\fcial upper bound on performance will be imposed by the branch resource constraints. For example, in an instruction stream consisting of $40\\%$ branches, a four issue processor capable of processing only one branch per cycle is bounded to a maximum of \u0002.\u0005 sustained instructions per cycle.  \n\nPredicated execution support provides an e\u000bective means to eliminate branches from an instruction stream. Predicated or guarded execution refers to the conditional execution of an instruction based on the value of a boolean source operand, referred to as the predicate [\u0007] [\b]. This architectural support allows the compiler to employ an $i f$ - conversion algorithm to convert conditional branches into predicate de\fning instructions, and instructions along alternative paths of each branch into predicated instructions [\t] [\u00010] [\u0001\u0001]. Predicated instructions are fetched regardless of their predicate value. Instructions whose predicate is true are executed normally. Conversely, instructions whose predicate is false are nulli\fed, and thus are prevented from modifying the processor state.  \n\nPredicated execution provides the opportunity to signi\f- cantly improve branch handling in ILP processors. The most obvious bene\ft is that decreasing the number of branches reduces the need to sustain multiple branches per cycle. Therefore, the arti\fcial performance bounds imposed by limited branch resources can be alleviated. Eliminating frequently mispredicted branches also leads to a substantial reduction in branch prediction misses [\u0001\u0002]. As a result, the performance penalties associated with mispredictions of the eliminated branches are removed. Finally, predicated execution provides an e\u000ecient interface for the compiler to expose multiple execution paths to the hardware. Without compiler support, the cost of maintaining multiple execution paths in hardware grows exponentially.  \n\nPredicated execution may be supported by a range of architectural extensions. The most complete approach is full predicate support. With this technique, all instructions are provided with an additional source operand to hold a predicate speci\fer. In this manner, every instruction may be a predicated. Additionally, a set of predicate de\fning opcodes are added to e\u000eciently manipulate predicate values. This approach was most notably utilized in the Cydra \u0005 minisupercomputer [\b] [\u0001\u0003]. Full predicate execution support provides the most \rexibility and the largest potential performance improvements. The other approach is to provide partial predicate support. With partial predicate support, a small number of instructions are provided which conditionally execute, such as a conditional move. As a result, partial predicate support minimizes the required changes to existing instruction set architectures (ISA's) and data paths. This approach is most attractive for designers extending current ISA's in an upward compatible manner.  \n\nIn this paper, the tradeo\u000bs involved in supporting full and partial predicated execution are investigated. Using the compilation techniques proposed in this paper, partial predicate support enables the compiler to perform full ifconversion to eliminate branches and expose ILP. Therefore, the compiler may remove as many branches with partial predicate support as with full predicate support. By removing a large portion of the branches, branch handling is signi\fcantly improved for ILP processors with partial predicate support. The relatively few changes needed to add partial predicate support into an architecture make this approach extremely attractive for designers.  \n\nHowever, there are several fundamental performance limitations of partial predicate support that are overcome with full predicate support. These di\u000eculties include representing unsupported predicated instructions, manipulating predicate values, and relying extensively on speculative execution. In the \frst case, for an architecture with only partial predicate support, predicated operations must be performed using an equivalent sequence of instructions. Generation of these sequences results in an increase in the number of instructions executed and requires a larger number of registers to hold intermediate values for the partial predicate architecture. In the second case, the computation of predicate values is highly e\u000ecient and parallel with full predicate support. However, this same computation with partial predicate support requires a chain of sequentially dependent instructions, that can frequently increase the critical path length. Finally, the performance of partial predicate support is extensively dependent on the use of speculative execution. Conditional computations are typically represented by \frst performing the computation unconditionally (speculative) and storing the result(s) in some temporary locations. Then, if the condition is true, the processor state is updated, using one or more conditional moves for example. With full predicate support, speculation is not required since all instructions may have a predicate speci\fer. Thus, speculation may be selectively employed where it improves performance rather than always being utilized.  \n\nThe issues discussed in the paper are intended for both designers of new ISA's, as well as those extending existing ISA's. With a new instruction set, the issue of supporting full or partial predicate support is clearly a choice that is available. Varying levels of partial predicate support provide options for extending an existing ISA. For example, introducing guard instructions which hold the predicate speci\fers of subsequent instructions may be utilized [\u0001\u0004].  \n\n# \u0002 ISA Extensions  \n\nIn this section, a set of extensions to the instruction set architecture for both full and partial predicate support are presented. The baseline architecture assumed is generic ILP processor (either VLIW or superscalar) with in-order issue and register interlocking. A generic load/store ISA is further assumed as the baseline ISA.  \n\n# \u0002.\u0001 Extensions for Full Predication  \n\nThe essence of predicated execution is the ability to suppress the modi\fcation of the processor state based upon some condition. There must be a way to express this condition and a way to express when the condition should a\u000bect execution. Full predication cleanly supports this through a combination of instruction set and micro-architecture extensions. These extensions can be classi\fed as support for suppression of execution and expression of condition.  \n\nSuppression of Execution. The result of the condition which determines if an instruction should modify state is stored in a set of \u0001-bit registers. These registers are collectively referred to as the predicate register \fle. The setting of these registers is discussed later in this section. The values in the predicate register \fle are associated with each instruction in the extended instruction set through the use of an additional source operand. This operand speci\fes which predicate register will determine whether the operation should modify processor state. If the value in the speci\fed predicate register is \u0001, or true, the instruction is executed normally; if the value is $0$ , or false, the instruction is suppressed.  \n\nOne way to perform the suppression of an instruction in hardware is to allow the instruction to execute and to disallow any change of processor state in the write-back stage of the pipeline. This method is useful since it reduces the latency between an instruction that modi\fes the value of the predicate register and a subsequent instruction which is conditioned based on that predicate register. This reduced latency enables more compact schedules to be generated for predicated code. A drawback to this method is that regardless of whether an instruction is suppressed, it still ties up an execution unit. This method may also increase the complexity of the register bypass logic and force exception signalling to be delayed until the last pipeline stage.  \n\nAn instruction can also be suppressed during the decode/issue stage. Thus, an instruction whose corresponding predicate register is false is simply not issued. This has the advantage of allowing the execution unit to be allocated to other operations. Since the value of the predicate register referenced must be available during decode/issue, the predicate register must at least be set in the previous cycle. This dependence distance may also be larger for deeper pipelines or if bypass is not available for predicate registers. Increasing the dependence distance between de\fnitions and uses of predicates may adversely a\u000bect execution time by lengthening the schedule for predicated code. An example of this suppression model is the predicate support provided by the Cydra \u0005 [\b]. Suppression at the decode/issue stage is also assumed in our simulation model.  \n\nTable \u0001: Predicate de\fnition truth table.   \n\n![](images/d88a67e03702cce7c5eb4ec5a153d1c6a7cefb8bc617b8a43643f1f1c916d5c8.jpg)  \n\nExpression of Condition. A set of new instructions is needed to set the predicate registers based upon conditional expressions. These instructions can be classi\fed as those that de\fne, clear, set, load, or store predicate registers.  \n\nPredicate register values may be set using predicate de\fne instructions. The predicate de\fne semantics used are those of the HPL Playdoh architecture [\u0001\u0005]. There is a predicate de\fne instruction for each comparison opcode in the original instruction set. The major di\u000berence with conventional comparison instructions is that these predicate de\fnes have up to two destination registers and that their destination registers are predicate registers. The instruction format of a predicate de\fne is shown below.  \n\npred <cmp> Pout\u0001 $_{<t y p e>}$ , $\\mathrm{Pout2}_{<t y p e>}$ , src\u0001, src\u0002 $(\\mathrm{P}_{i n}$ )  \n\nThis instruction assigns values to Pout\u0001 and Pout\u0002 according to a comparison of src\u0001 and src\u0002 speci\fed by $<c m p>$ . The comparison $<c m p>$ can be: equal (eq), not equal (ne), greater than (gt), etc. A predicate $<t y p e>$ is speci\fed for each destination predicate. Predicate de\fning instructions are also predicated, as speci\fed by $P_{i n}$ .  \n\nThe predicate $<t y p e>$ determines the value written to the destination predicate register based upon the result of the comparison and of the input predicate, $P_{i n}$ . For each combination of comparison result and $P_{i n}$ , one of three actions may be performed on the destination predicate. It can write \u0001, write 0, or leave it unchanged. A total of $3^{4}=81$ possible types exist.  \n\nThere are six predicate types which are particularly useful, the unconditional $U$ ), $O R$ , and $A N D$ type predicates and their complements. Table \u0001 contains the truth table for these predicate types.  \n\nUnconditional destination predicate registers are always de\fned, regardless of the value of $P_{i n}$ and the result of the comparison. If the value of $P_{i n}$ is \u0001, the result of the comparison is placed in the predicate register (or its compliment for $\\overline{U}$ ). Otherwise, a 0 is written to the predicate register. Unconditional predicates are utilized for blocks which are executed based on a single condition, i.e., they have a single control dependence.  \n\nThe $O R$ type predicates are useful when execution of a block can be enabled by multiple conditions, such as logical AND (&&) and OR (k) constructs in C. $O R$ type destination predicate registers are set if $P_{i n}$ is \u0001 and the result of the comparison is \u0001 ( $0$ for $\\overline{{O R}}$ ), otherwise the destination predicate register is unchanged. Note that $O R$ type predicates must be explicitly initialized to $0$ before they are de\fned and used. However, after they are initialized multiple $O R$ type predicate de\fnes may be issued simultaneously and in any order on the same predicate register. This is true since the $O R$ type predicate either writes a \u0001 or leaves the register unchanged which allows implementation as a wired logical $O R$ condition. This property can be utilized to compute an execution condition with zero dependence height using multiple predicate de\fne instructions.  \n\n![](images/1ce01e64fb9e2dfa783553b1ddd9ded6b011763d9b82c584821e369aeb99eb7c.jpg)  \nFigure \u0001: Example of predication, (a) source code, (b) assembly code, (c) assembly code after if-conversion.  \n\n$A N D$ type predicates, are analogous to the $O R$ type predicate. $A N D$ type destination predicate registers are cleared if $P_{i n}$ is \u0001 and the result of the comparison is 0 (\u0001 for $A N D$ ), otherwise the destination predicate register is unchanged. The $A N D$ type predicate is particularly useful for transformations such as control height reduction [\u0001\u0006].  \n\nAlthough it is possible to individually set each predicate register to zero or one through the use of the aforementioned predicate de\fne instructions, in some cases individually setting each predicate can be costly. Therefore, two instructions, pred clear and pred set, are de\fned to provide a method of setting the entire predicate register \fle to $0$ or \u0001 in one cycle.  \n\nCode Example. Figure \u0001 contains a simple example illustrating the concept of predicated execution. The source code in Figure \u0001(a) is compiled into the code shown in Figure \u0001(b). Using if-conversion [\u00010], the code is then transformed into the code shown in Figure \u0001(c). The use of predicate registers is initiated by a pred clear in order to insure that all predicate registers are cleared. The \frst two conditional branches in (b) are translated into two pred eq instructions. Predicate register $p1$ is $O R$ type since either condition can be true for $p\\*{}$ to be true. If $p\\mathscr{Q}$ in the \frst pred eq is false the second pred eq is not executed. This is consistent with short circuit boolean evaluation. $p3$ is true only if the entire expression is true. The \\then\" part of the outer if statement is predicated on $p\\mathcal{3}$ for this reason. The pred ne simply decides whether the addition or subtraction instruction is performed. Notice that both $p4$ and $p5$ remain at zero if the pred ne is not executed. This is consistent with the \\else\" part of the outer if statement. Finally, the increment of $i$ is performed unconditionally.  \n\n# \u0002.\u0002 Extensions for Partial Predication  \n\nEnhancing an existing ISA to support only partial predication in the form of conditional move or select instructions trades o\u000b the \rexibility and e\u000eciency provided by full predication in order to minimize the impact to the ISA. Several existing architectures provide instruction set features that re\rect this point of view.  \n\nConditional Move. The conditional move instruction provides a natural way to add partial support for predicated execution to an existing ISA. A conditional move instruction has two source operands and one destination operand, which ts well into current \u0003 operand ISA's. The semantics of a conditional move instruction, shown below, are similar to that of a predicated move instruction.  \n\n# cmov dest,src,cond if ( cond ) dest = src  \n\nAs with a predicated move, the contents of the source register are copied to the destination register if the condition is true. Also, the conditional modi\fcation of the target register in a conditional move instruction allows simultaneous issue of conditional move instructions having the same target register and opposite conditions on an in-order processor. The principal di\u000berence between a conditional move instruction and a predicated move instruction is that a register from the integer or \roating-point register \fle is used to hold the condition, rather than a special predicate register le. When conditional moves are available, we also assume conditional move complement instructions (cmov com) are present. These are analogous in operation to conditional moves, except they perform the move when cond is false, as opposed to when cond is true.  \n\nThe Sparc V\t instruction set speci\fcation and the DEC Alpha provide conditional move instructions for both integer and \roating point registers. The HP Precision Architecture [\u0001\u0007] provides all branch, arithmetic, and logic instructions the capability to conditionally nullify the subsequent instruction. Currently the generation of conditional move instructions is very limited in most compilers. One exception is the DEC GEM compiler that can e\u000eciently generate conditional moves for simple control constructs [\u0001\b].  \n\nSelect. The select instruction provides more \rexibility than the conditional move instruction at the expense of pipeline implementation. The added \rexibility and increased di\u000eculty of implementation is caused by the addition of a third source operand. The semantics of the select instruction are shown below.  \n\n# select dest,src\u0001,src\u0002,cond  \n\nUnlike the conditional move instruction, the destination register is always modi\fed with a select. If the condition is true, the contents of src\u0001 are copied to the destination, otherwise the contents of src\u0002 are copied to the destination register. The ability to choose one of two values to place in the destination register allows the compiler to e\u000bectively choose between computations from \\then\" and \\else\" paths of conditionals based upon the result of the appropriate comparison. As a result, select instructions enable more e\u000ecient transformations by the compiler. This will be discussed in more detail in the next section. The Multi\row Trace \u000300 series machines supported partial predicated execution with select instructions [\u0001\t].  \n\n# \u0003 Compiler Support  \n\nThe compiler eliminates branch instructions by introducing conditional instructions. The basic transformation is known as if-conversion [\t] [\u00010]. In our approach, full predicate support is assumed in the intermediate representation (IR) regardless of the the actual architectural support in the target processor. A set of compilation techniques based on the hyperblock structure are employed to e\u000bectively exploit predicate support in the IR [\u0001\u0001]. For target processors that only have partial predicate support, unsupported predicated instructions are broken down into sequences of equivalent instructions that are representable. Since the transformation may introduce ine\u000eciencies, a comprehensive set of peephole optimizations is applied to code both before and after conversion. This approach of compiling for processors with partial predicate support di\u000bers from conventional code generation techniques. Conventional compilers typically transform simple control \row structures or identify special patterns that can utilize conditional moves or selects. Conversely, the approach utilized in this paper enables full if-conversion to be applied with partial predicate support to eliminate control \row.  \n\nIn this section, the hyperblock compilation techniques for full predicate support are \frst summarized. Then, the transformation techniques to generate partial predicate code from a full predicate IR are described. Finally, two examples from the benchmark programs studied are presented to compare and contrast the e\u000bectiveness of full and partial predicate support using the these compilation techniques.  \n\n# \u0003.\u0001 Compiler Support for Full Predication  \n\nThe compilation techniques utilized in this paper to exploit predicated execution are based on a structure called a hyperblock [\u0001\u0001]. A hyperblock is a collection of connected basic blocks in which control may only enter at the \frst block, designated as the entry block. Control \row may leave from one or more blocks in the hyperblock. All control \row between basic blocks in a hyperblock is eliminated via if-conversion. The goal of hyperblocks is to intelligently group basic blocks from many di\u000berent control \row paths into a single block for compiler optimization and scheduling.  \n\nBasic blocks are systematically included in a hyperblock based on two, possibly con\ricting, high level goals. First, performance is maximized when the hyperblock captures a large fraction of the likely control \row paths. Thus, any blocks to which control is likely to \row are desirable to add to the hyperblock. Second, resource (fetch bandwidth and function units) are limited; therefore, including too many blocks may over saturate the processor causing an overall performance loss. Also, including a block which has a comparatively large dependence height or contains a hazardous instruction (e.g., a subroutine call) is likely to result in performance loss. The \fnal hyperblock consists of a linear sequence of predicated instructions. Additionally, there are explicit exit branch instructions (possibly predicated) to any blocks not selected for inclusion in the hyperblock. These branch instructions represent the control \row that was identi\fed as unpro\ftable to eliminate with predicated execution support.  \n\n![](images/556c9b65f2c1ff3e8163935c497827ebab5698dc9cef64d1e3147570bef0fef8.jpg)  \nFigure \u0002: Example of predicate promotion.  \n\n# \u0003.\u0002 Compiler Support for Partial Predication  \n\nGenerating partially predicated code from fully predicated code involves removing predicates from all instructions which are not allowed to have a predicate speci\fer. The only instruction set remnants of predication in the partially predicated code are conditional move or select instructions. Transforming fully predicated code to partially predicated code is essentially accomplished by converting predicated instructions into speculative instructions which write to some temporary location. Then, conditional move or select instructions are inserted to conditionally update the processor state based on the value of the predicate. Since all predicated instructions are converted to speculative instructions, the e\u000eciency of the partially predicated code is heavily dependent on the underlying support for speculation provided by the processor. In this section, the code generation procedure chosen to implement the full to partial predication transformation is described. The procedure is divided into \u0003 steps, predicate promotion, basic conversion, and peephole optimization.  \n\nPredicate Promotion. The conversion of predicated instructions into an equivalent set of instructions that only utilize conditional moves or selects introduces a signi\fcant amount of code expansion. This code expansion is obviously reduced if there are fewer predicated instructions that must be converted. Predicate promotion refers to removing the predicate from a predicated instruction [\u0001\u0001]. As a result, the instruction is unconditionally executed. By performing predicate promotion, fewer predicated instructions remain in the IR that must be converted.  \n\nAn example to illustrate the e\u000bectiveness of predicate promotion is presented in Figure \u0002. The code sequence in the upper left box is the original fully predicated IR. Straightforward conversion to conditional move code, as will be discussed in the next subsection, yields the code in the upper right box. Each predicated instruction is expanded into two instructions for the partial predicate architecture. All the conditional moves in this sequence, except for the last, are unnecessary if the original destination registers of the predicated instructions are temporary registers. In this case, the predicate of the \frst two instructions can be promoted, as shown in the lower left box of Figure \u0002. The add instruction is the only remaining predicated instruction. Finally, conversion to conditional move code after promotion yields the code sequence in the bottom right box of Figure \u0002. In all, the number of instructions is reduced from \u0006 to \u0004 in this example with predicate promotion.  \n\nIt should be noted that predicate promotion is also e\u000bective for architectures with full predicate support. Predicate promotion enables speculative execution by allowing predicated instructions to execute before their predicate is calculated. In this manner, the dependence between the predicate de\fnition and the predicated instruction is eliminated. The hyperblock optimizer and scheduler utilize predicate promotion when the predicate calculation occurs along a critical dependence chain to reduce this dependence length.  \n\nBasic Conversions. In the second step of the transformation from fully predicated code to partially predicated code, a set of simple transformations, referred to as basic conversions, are applied to each remaining predicated instruction independently. The purpose of the basic conversions is to replace each predicated instruction by a sequence of instructions with equivalent functionality. The sequence is limited to contain conditional moves as the only conditional instructions. As a result, most instructions in the sequence must be executed without a predicate. These instructions thus become speculative. When generating speculative instructions, the compiler must ensure they only modify temporary registers or memory locations. Furthermore, the compiler must ensure the speculative instructions will not cause any program terminating exceptions when the condition turns out to be false. Program terminating exceptions include illegal memory address, divide-by-zero, over\row, or under\row.  \n\nThe basic conversions that may be applied are greatly simpli\fed if the underlying processor has support full support for speculative execution. In particular, non-excepting or silent, instructions allow for the most e\u000ecient transformations. For such an architecture, the basic conversions for the main classes of instructions are summarized in Figure \u0003. The simplest conversion is used for predicated arithmetic and logic instructions and also for memory loads. The conversion, as can be seen in Figure \u0003, is to rename the destination of the predicated instruction, remove the predicate, and then conditionally move the result into the original destination based on the result of the predicate.  \n\nThe basic conversions for memory store instructions are similar. Since the destination of a store instruction is a memory location instead of a register, a di\u000berent technique must be used to insure that the an invalid value is not written to the original destination of the store. Figure \u0003 shows that the address of the store is calculated separately. Then a conditional move is used to replace the address of the store with \\$safe addr when the predicate of the store is false. The macro \\$safe addr refers to a reserved location on the stack.  \n\nThe conversions for predicate de\fnition instructions are the most complicated because predicate de\fnitions have rather complicated logic capabilities. The conversions for two representative predicate de\fnition instructions are shown in Figure \u0003. The predicate de\fnition instructions are identical except for the type on the destination predicate register. The transformation for the $O R$ type predicate produces three instructions. The \frst instruction performs the $\\mathit{l t}$ comparison of src\u0001 and src\u0002 , placing the result in a temporary register. Each predicate de\fnition transformation generates such a comparison instruction. The second instruction performs a logical $A N D$ which clears the temporary register if the predicate Pin is false. This clearing instruction is generated only if the predicate de\fnition instruction is predicated. The third instruction performs a logical $O R$ of the value in the temporary register with the previous value of the $O R$ type predicate Pout and deposits the result t in Pout. For an $A N D$ type predicate, the result would be stored with a logical $A N D$ . For an unconditional predicate, a separate depositing instruction is not necessary.  \n\n![](images/11cfe2993145b3c8b7a677bd320648f4dd1afaf64b65e32417d365435f9bd6bb.jpg)  \n\nThe basic conversions for branches are relatively straight forward and are left to the reader. Predicated subroutine calls are handled by branching around them when the predicate is false since conditional calls were not assumed in the architecture.  \n\nConversions are also possible if no speculation support is provided. However, in addition to insuring that registers or memory locations are not illegally modi\fed, the basic conversions must also prevent exceptions when the original predicate is false. Figure \u0004 shows three typical conversions. The non-excepting versions of these appeared in Figure \u0003. Note that the excepting versions produce more instructions than the corresponding conversions for non-excepting instructions. For predicate de\fnition, arithmetic, and logic instructions, the only di\u000berence in the conversions is that a value that is known to prevent an exception is conditionally moved into one of the source operands of the previously predicated instruction. These values, which depend on the type of instruction, are referred to as \\$safe val in the \fgure. The conversions for \roating point conditional branch instructions are similar. Conversion for load instructions is also similar, only an address known not to cause an illegal memory access is moved into the address source of the load.  \n\n![](images/4d3902f6a57caed967b9eb799f78df8ff0d6fd3d47f2ad3d18e7b6269e666976.jpg)  \n\nPeephole Optimizations. The basic transformations of the previous section introduce some ine\u000eciencies since each instruction is considered independently. Many of these ine\u000eciencies can be removed by applying a set of peephole optimizations after the basic transformation. The goal of these optimizations is to reduce the instruction count and dependence height of the partial predicate code. The optimizations \fnd opportunities for improving code e\u000eciency by investigating the interactions of the various transformations, exploiting special cases, and utilizing the additional functionality of the select instruction over the conditional move. Some of the optimizations in this section rely on the existence of complementary $A N D$ and $O R$ instructions (and not and or not). These instructions are simply logical instructions in which the second source operand is complemented. The existence of these instructions is assumed in the base instruction set.  \n\nBasic conversions of predicate de\fnes introduce redundant comparison and logic instructions. For predicates which only di\u000ber in predicate type $(U,O R,A N D)$ , the comparisons are obviously redundant. Applying common subexpression elimination, copy propagation, and dead code removal after conversion e\u000bectively eliminates these redundancies. In some cases, the transformations of similar predicate de\fnitions result in opposite comparisons. If one of these comparisons can be inverted, then one of the comparisons may be eliminated. A comparison can be inverted when each use of the result of this comparison can be inverted without the addition of an instruction. The result of a comparison in a predicate de\fnition instruction used only by and, and not, or, or not, cmov, cmov com, select, or a conditional branch may be inverted. The only two non-invertible sources which might contain the result of a predicate de\fnition conversion are the non-inverted inputs of and not and or not. Therefore, in most cases, one of two complementary comparisons resulting from similar predicate de\fnitions can be eliminated.  \n\nThe use of $O R$ type predicates is extremely e\u000ecient for architectures with full predicate support. Sequences of $O R$ type predicate de\fnitions which all write to the same destination predicate may be simultaneously executed. However, with partial support, these sequences of $O R$ type predicate de\fnitions result in a sequential chain of dependent instructions. These strict sequential dependences may be overcome using associativity rules to reduce the height of the dependence chain. The dependence height of the resulting code is $l o g_{2}(n)$ , where n is the number of $O R$ type predicate de\fnitions. An example of OR-Tree optimization is presented in Section \u0003.\u0003.  \n\nSome additional optimizations are possible if a select instruction is available. The functionality of the select instruction is described in Section \u0002.\u0002. Through the use of a select instruction, one instruction from the sequences used for excepting arithmetic and memory instructions shown in Figure \u0004 can be eliminated. The detailed use of selects is not discussed in this paper due to space considerations.  \n\n# \u0003.\u0003 Benchmark Examples  \n\nIn order to more clearly understand the e\u000bectiveness of predicated execution support and the performance tradeo\u000bs of full versus partial support, two examples from the set of benchmarks are presented. The \frst example is from $w c$ and the second is from grep. These benchmarks were chosen because they are relatively small, yet they are very controlintensive so they clearly illustrate the e\u000bectiveness of full and partial predicate support.  \n\nExample Loop from Wc. Figure \u0005(a) shows the control \row graph for the most important loop segment from the benchmark $w c$ . The control \row graph is augmented with the execution frequencies of each control transfer for the measured run of the program. This loop is characterized by small basic blocks and a large percentage of branches. The loop segment contains \u0001\u0003 basic blocks with a total of \u0003\u0004 instructions, \u0001\u0004 of which are branches. The performance of an \b-issue ILP processor without predicated execution support is limited by this high frequency of branches. Overall, a speedup of \u0002.\u0003 is achieved for an \b-issue processor over a \u0001-issue processor (see Figure \b).  \n\nThe assembly code after hyperblock formation for the loop segment with full and partial predicate support is shown in Figures \u0005(b) and (c), respectively. The issue cycle is given to the right of each assembly code instruction. Note that the assembly code is not reordered based on the issue cycle for ease of understanding. The schedule assumes a \u0004-issue processor which can issue \u0004 instructions of any type except branches, which are limited to \u0001 per cycle. With both full and partial predicate support, all of the branches except three are eliminated using hyperblock formation. The three remaining branches, conditional branch to block C, conditional branch to EXIT, and the loop backedge, are highly predictable. Therefore, virtually all the mispredictions are eliminated with both full and partial predicate support in this loop. The resulting performance is increased by $17\\%$ with partial predicate support and an additional $88\\%$ with full predicate support (see Figure \b).  \n\nThe performance di\u000berence between full and partial predicate support comes from the extra instructions required to represent predicate de\fnes and predicated instructions. As a result, the issue resources of the processor are over saturated with partial predicate support. In the example in Figure \u0005, the number of instructions is increased from \u0001\b with full predicate support to \u0003\u0001 with partial predicate support. This results in an increase in execution time from \b to \u00010 cycles. For the entire benchmark execution, a similar trend is observed. The number of instructions is increased from \u0001\u0005\u0002\u0006K with full predicate support to \u0002\t\t\tK with partial predicate support, resulting in a speedup increase of \u0002.\u0007 with partial support to \u0005.\u0001 with full support (see Figure \b and Table \u0002).  \n\nExample Loop from Grep. Figure \u0006 shows the assembly code for the most important loop segment from the benchmark grep. The base processor model, which does not support any predicated execution, employs speculative execution in conjunction with superblock ILP compilation techniques to achieve the schedule shown in Figure \u0006(a) [\u00020]. Each of the conditional branches in the \fgure are very infrequently taken, thus the sequence of instructions iterates very frequently. Overall, grep is dominated by an extremely high frequency of branches. This high frequency of branches is the performance bottleneck of this loop since only \u0001 branch resource is available. However, the branches are highly predictable. Thus, hyperblock compilation techniques focus on reducing this branch bottleneck for processors with limited branch resources.  \n\nWith full predicate support, the compiler is able to combine the branches into a single exit branch using $O R$ type predicate de\fnitions. Since $O R$ type predicate de\fnitions can be issued simultaneously, an extremely tight schedule can be achieved. The execution time is dramatically reduced from \u0001\u0004 to \u0006 cycles with full predicate support. With partial predicate support, the same transformations are applied. Therefore, the same number of branches are eliminated. However, the representation of $O R$ type predicates is less e\u000ecient with partial predicate support. In particular, the logical OR instructions cannot be simultaneously issued. The or-tree optimization discussed previously in Section \u0003.\u0002 is applied to reduce the dependence height of the sequence and improve performance. In the example, partial predicate support improves performance from \u0001\u0004 to \u00010 cycles. Overall for the \fnal benchmark performance, partial predicate support improves performance by $46\\%$ over the base code and full predicate support further improves performance by $31\\%$ .  \n\n# \u0004 Experimental Evaluation  \n\n# \u0004.\u0001 Methodology  \n\nThe predication techniques presented in this paper are evaluated through emulation-driven simulation. The benchmarks studied consist of 00\b.espresso, 0\u0002\u0002.li, 0\u0002\u0003.eqntott, 0\u0002\u0006.compress, 0\u0005\u0002.alvinn, 0\u0005\u0006.ear, and 0\u0007\u0002.sc from SPEC-\t\u0002, and the Unix utilities cccp, cmp, eqn, grep, lex, qsort, wc, and yacc. The benchmark programs are initially compiled to produce intermediate code, which is essentially the instruction set of an architecture with varying levels of support for predicated execution. Register allocation and code scheduling are performed in order to produce code that could be executed by a target architecture with such support. To allow emulation of the code on the host HP PA-RISC processor, the code must be modi\fed to remove predication, while providing accurate emulation of predicated instructions.  \n\n![](images/7f178fed14e3cb60d3a0cdf8ebb6bb42955cdddb49cebf307bf24fa5633e7c8f.jpg)  \nFigure \u0005: Example loop segment from wc.  \n\nEmulation ensures that the optimized code generated for each con\fguration executes correctly. Execution of the benchmark with emulation also generates an instruction trace containing memory address information, predicate register contents, and branch directions. This trace is fed to a simulator for performance analysis of the particular architectural model being studied. We refer to this technique as emulation-driven simulation. The simulator models, in detail, the architecture's prefetch and issue unit, instruction and data caches, branch target bu\u000ber, and hardware interlocks, providing an accurate measure of performance.  \n\nPredicate Emulation. Emulation is achieved by performing a second phase of register allocation and generating PA-RISC assembly code. The emulation of the varying levels of predicate support, as well as speculation of load instructions is done using the bit manipulation and conditional nulli\fcation capabilities of the PA-RISC instruction set [\u0001\u0007]. Predicates are emulated by reserving $_n$ of the callee-saved registers and accessing them as $32\\times n$ \u0001-bit registers.  \n\nThe instruction sequence required to emulate a predicate de\fne instruction is dependent upon the predicate types of the destination predicate registers. As an example, consider the predicated predicate de\fne instruction (\u0001) in Figure \u0007. In this example, predicate registers $p1$ , $p\\mathscr{Q}$ , and $p\\mathcal{3}$ have been assigned bits \u0001,\u0002, and \u0003 of general register $\\%r3$ , respectively. Instruction (\u0001) is de\fning predicate register $p1$ as $O R$ type and $p3$ as unconditional complement. The \frst instruction in the \fve instruction assembly code sequence, places a 0 in bit \u0003 of register $\\%r3$ , unconditionally setting $p3$ to 0. The second instruction will branch around the remaining instructions if the predicate $p\\mathscr{Q}$ is 0. If $p\\mathscr{Q}$ is \u0001 the third instruction then performs the comparison, and using the conditional nulli\fcation capabilities of that instruction, determines which of the next two instructions will be executed. If the contents of $\\%r{\\mathcal{Q}}{\\mathcal{L}}$ is $0$ , then only the \ffth instruction will be executed, writing a \u0001 to bit \u0001 of $\\%r3$ , setting $p I$ to \u0001. Otherwise, only the fourth instruction will be executed, writing a \u0001 to bit \u0003 of $\\%r3$ , setting $p\\mathcal{B}$ to \u0001.  \n\nPredicated instructions are emulated by extracting the bit from one of the reserved registers that corresponds to the predicate for that instruction. The value of that bit is used to conditionally execute the predicated instruction. For example, instruction (\u0002) in Figure \u0007 is predicated on $p\\mathcal{B}$ . Thus, bit \u0003 is extracted from $\\%r\\mathcal{B}$ and is used to conditionally nullify the increment of $\\%r{\\mathcal{Q}}{\\it5}$ .  \n\nConditional Move Emulation. The emulation of conditional move and select instructions is done in a similar fashion. Instruction (\u0003) in Figure \u0007 is a conditional move of $r6$ into $r5$ if the contents of $r\\boldsymbol{\\delta}$ is non-zero. Emulation requires two instructions. The \frst performs the comparison and nulli\fes the subsequent copy of $\\textit{6}$ into $\\it5$ if $r{\\boldsymbol{\\delta}}$ is zero. Instruction (\u0004) in Figure \u0007 is a select instruction. As with the conditional move instruction, the \frst instruction performs a comparison to determine the contents of $^{r\\delta}$ . If $^{r\\delta}$ is zero, $r7$ will be copied into $r5$ , otherwise $r\\boldsymbol{\\theta}$ is copied into $r5$ as described in Section \u0002.\u0002.  \n\nProcessor Models. Three processor models are evaluated this paper. The baseline processor is a k-issue processor, with no limitation placed on the combination of instructions which may be issued each cycle, except for branches. The memory system is speci\fed as either perfect or consists of a \u0006\u0004K directed mapped instruction cache and a \u0006\u0004K direct mapped, blocking data cache; both with \u0006\u0004 byte blocks. The data cache is write-through with no write allocate and has a miss penalty of \u0001\u0002 cycles. The dynamic branch prediction strategy employed is a \u0001K entry BTB with \u0002 bit counter with a \u0002 cycle misprediction penalty. The instruction latencies assumed are those of the HP PA-RISC \u0007\u000100. Lastly, the baseline processor is assumed to have an in\fnite number of registers. The baseline processor does not support any form of predicated execution. However, it includes non-excepting or silent versions of all instructions to fully support speculative execution. Superblock ILP compilation techniques are utilized to support the baseline processor [\u00020]. The baseline processor is referred to as Superblock in all graphs and tables.  \n\n![](images/80a55870926b7a9c8e14f8bc93e006d96bc9833625ab4cbaf92ef7e35ae16c9a.jpg)  \nFigure \u0006: Example loop segment from grep.  \n\nFor partial predicate support, the baseline processor is extended to support conditional move instructions. Note that since non-excepting versions of all instructions are available, the more e\u000ecient conversions are applied by the compiler for partial predication (Section \u0003.\u0002). The partial predicate support processor is referred to as Conditional Move. The nal model is the baseline processor extended to support full predication as described in Section \u0002.\u0001. This model is referred to as Full Predication. For this model, hyperblock compilation techniques are applied. Performance of the \u0003 models is compared by reporting the speedup of the particular processor model versus the baseline processor. In particular, speedup is calculated by dividing the cycle count for a \u0001-issue baseline processor by the cycle count of a k-issue processor of the speci\fed model.  \n\n![](images/390619482cb7d589caf2d2fca31b56cd6114d1fe06f0b8cc7bbe252cf046e89a.jpg)  \nFigure \u0007: HP PA-RISC emulation of predicate support.  \n\n# \u0004.\u0002 Results  \n\nFigure \b shows the relative performance achieved by superblock, conditional move, and full predication for an issue\b, \u0001-branch processor. Full predication performed the best in every benchmark with an average speedup of $63\\%$ over superblock.\u0001 Speedup with conditional move code fell between superblock and full predication for all benchmarks except 0\u0007\u0002.sc which performed slightly below superblock code's performance. The unusual behavior of $072.s c$ was primarily due to increased dependence chain lengths caused by the conditional move transformations. On average, though, conditional move code had a speedup of $33\\%$ over superblock. The speedup for conditional move code is very substantial. Most researchers and product developers have reported small gains except for certain special cases with conditional moves. However, utilizing the hyperblock techniques in conjunction with the conditional move transformations yields consistent performance improvements.  \n\n![](images/78a8b06ff424fb4f82234335a6b80728038192a551588e76535b8cfb3a41a260.jpg)  \nFigure \b: E\u000bectiveness of full and partial predicate support for an \b-issue, \u0001-branch processor with perfect caches.  \n\n![](images/2e624d6d300b0a4e52f18cbc646cb79b7e65ea2e9bab837f722ef8a6190eadbc.jpg)  \nFigure \t: E\u000bectiveness of full and partial predicate support for an \b-issue, \u0002-branch processor with perfect caches.  \n\nFull predication also achieved performance gain on top of the conditional move model. This illustrates that there is signi\fcant performance gain possible provided by the ISA changes to support full predication. In particular, the e\u000e- ciency of representing predicated instructions, the reduced dependence heights to represent predicated instructions, and the ability to simultaneously execute $O R$ type predicate denes provided full predicate support with the additional performance improvement. On average, a gain of $30\\%$ over the conditional move model was observed.  \n\nIncreasing the branch issue rate from \u0001 to \u0002 branches per cycle provides interesting insight into the e\u000bectiveness of predicated execution. Figure \t shows the performance result of an \b-issue processor that can execute \u0002 branches per cycle. The performance improvement of conditional move code and full predicate against superblock code is reduced. This is attributal to improving the performance of superblock. The conditional move and full predication code has had many of the branches removed with hyperblock formation. Therefore, increasing the number of branches does not noticeably improve the performance of conditional move and full predication code. On average, conditional move performed only $3\\%$ faster than superblock while full predication performed $35\\%$ faster than superblock.  \n\n![](images/854a859a2b49ba5d3fbc8dda45683391126b9785646486fdf7dd988500183608.jpg)  \nFigure \u00010: E\u000bectiveness of full and partial predicate support for an \u0004-issue, \u0001-branch processor with perfect caches.  \n\n![](images/353037489ca196c305ace4987c40f825fb1b1d062af576e3ebb979fb4bcd2fbe.jpg)  \nFigure \u0001\u0001: E\u000bectiveness of full and partial predicate support for an \b-issue, \u0001-branch processor with \u0006\u0004K instruction and data caches.  \n\nFigure \u00010 shows performance of the benchmarks on a \u0004 issue processor that can issue \u0001 branch per cycle. The most noticeable trend across these benchmarks is that while full predication consistently beats superblock code, conditional move code performs worse than superblock in the majority of benchmarks. Since support for predication in the condition move code is limited, the compiler must compensate by creating many more instructions than it would with full predicate support. These extra instructions are absorbed by the \b issue machine, but saturate the \u0004 issue machine creating poor results. These results indicate a more conservative hyperblock formation algorithm needs to be employed for the conditional move model with a \u0004-issue processor. For full predication, substantial performance gain is still possible for the \u0004-issue processor, with an average of $33\\%$ speedup over superblock.  \n\nTo evaluate the cache e\u000bects associated with predicated execution, Figure \u0001\u0001 is presented. As expected all three methods were a\u000bected by a realistic cache model. However, two benchmarks stand out. The real cache signi\fcantly reduced the performance of 0\u0002\u0006.compress in all three models. Conditional move and full predication code increased data memory tra\u000ec more by performing speculative execution using predicate promotion. Since these promoted instructions often caused cache misses, the performance of conditional move and full predication code dropped signi\fcantly. $E q n$ also exhibited an interesting result. Conditional move performed poorly while full predication and superblock remained proportionally the same. This is a side e\u000bect of the increased instruction cache miss rate due to conditional move's larger instruction count. This is evidenced by the dynamic instruction count of eqn in Table \u0002. On average predicated code still yielded good results over superblock with full predication performing $54\\%$ faster than superblock and conditional moves performing $24\\%$ faster than superblock.  \n\nTable \u0002: Dynamic instruction count comparison.   \n\n![](images/ac3c4e16a7c3dd50c0ee3e59ca2dc14cf44f55ea30b1515e585253d9e16ac6c4.jpg)  \n\nThe dynamic instruction count of all benchmarks with respect to a processor model is shown in Table \u0002. Full predication code can increase dynamic instruction count over superblock as is executes both paths of an if-then-else construct. Superblock code can increase dynamic instruction count over full predication by unnecessary speculated instructions into frequently executed paths. Therefore the overall relation in instruction count between full predication and superblock can vary as the results indicate. Conditional move code's dynamic instruction count is hit hardest; however, since it su\u000bers from executing code normally hidden by branches combined with the ine\u000eciencies associated with not having full predicate support. Conditional move code had an average of $46\\%$ more dynamic instructions than superblock, while full predication had only \u0007% dynamic instruction.  \n\nFinally, as shown in Table \u0003, the number of branches in partially and fully predicated code is substantially less than in the superblock code. Much of the speedup of full and partial predication comes from the elimination of branches. Mispredicted branches incur a signi\fcant performance penalty. With fewer branches in the code, there are fewer mispredictions. Also, in many architectures, because of the high cost of branch prediction, the issue rate for branches is less than the issue rate for other instructions. Therefore, fewer branches in the code can greatly increase the available ILP. Partially and fully predicated code have very close to the same number of branches, with fully predicated code often having just a few less. The small di\u000berence in the number of branches is a result of adding branches around predicated subroutine calls in partially predicated code. The di\u000berences in the misprediction ratios for partially and fully predicated code is also a result of predicated subroutine calls.  \n\nAn odd behavior is observed for grep in Table \u0003. The number of mispredictions for the conditional move and full predication models are larger than that of the superblock model. This is caused by a branch combining transformation employed for hyperblocks by the compiler which is heavily applied for grep. With this transformation, unlikely taken branches are combined to a single branch. The goal of the transformation is to reduce the number of dynamic branches. However, the combined branch typically causes more mispredictions than the sum of the mispredictions caused by the original branches. As a result, the total number of mispredictions may be increased with this technique.  \n\n# \u0005 Concluding Remarks  \n\nThe code generation strategy presented in this paper illustrates the qualitative bene\ft of both partial and full predication. In general, both allow the compiler to remove a substantial number of branches from the instruction stream. However, full predication allows more e\u000ecient predicate evaluation, less reliance on speculative execution, and fewer instructions executed. As shown in our quantitative results, these bene\fts enable full predication to provide more robust performance gain in a variety of processor con\fgurations.  \n\nFor an eight issue processor that executes up to one branch per cycle, we show that conditional move allows about $30\\%$ performance gain over an aggressive base ILP processor with no predication support. This speedup is very encouraging and shows that a relatively small architectural extension can provided signi\fcant performance gain. Full predication o\u000bers another $30\\%$ gain over conditional move. The performance gains of full and partial predication support illustrate the importance of improving branch handling in ILP processors using predicated execution.  \n\nResults based on a four issue processor illustrate the advantage of full predication support. Full predication support remains substantially superior even in the presence of a low issue rate. This is due to its e\u000ecient predicate evaluation and low instruction overhead. This contrasts with conditional move support where the extra dynamic instructions over utilize the processor issue resources and result in a sizable performance degradation for the majority of the benchmarks. Nevertheless, the substantial performance gain for two of the benchmarks suggests that conditional move could be a valuable feature even in a low issue rate processor. However, this does indicate that a compiler must be extremely intelligent when exploiting conditional move on low issue rate processors.  \n\nAll of the results presented in this paper are based on a \u0002-cycle branch prediction miss penalty. This was chosen to show conservative performance gains for predicated execution. For machines with larger branch prediction miss penalties, we expect the bene\fts of both full and partial prediction to be much more pronounced. Furthermore, when more advanced compiler optimization techniques become available, we expect the performance gain of both partial and full predication to increase. We also feel it would be interesting to explore the range of predication support between conditional move and full predication support.  \n\nTable \u0003: Comparison of branch statistics: number of branches (BR), mispredictions (MP), and miss prediction rate (MPR).   \n\n![](images/4fdade8e8e672ec792aa684256a37883123fb61a40e911e1bdd2ab6666f0b6fa.jpg)  \n\n# Acknowledgements  \n\nThe authors would like to thank Roger Bringmann and Dan Lavery for their e\u000bort in helping put this paper together. We also wish to extend thanks to Mike Schlansker and Vinod Kathail at HP Labs for their insightful discussions of the Playdoh model of predicated execution. Finally, we would like to thank Robert Cohn and Geo\u000b Lowney at DEC, and John Ruttenberg at SGI for their discussions on the use of conditional moves and selects. This research has been supported by the National Science Foundation (NSF) under grant MIP-\t\u00030\b0\u0001\u0003, Intel Corporation, Advanced Micro Devces, Hewlett-Packard, SUN Microsystems and AT&T GIS.  \n\n# References  \n\n[\u0001] J. E. Smith, \\A study of branch prediction strategies,\" in Proceedings of the \bth International Symposium on Computer Architecture, pp. \u0001\u0003\u0005{\u0001\u0004\b, May \u0001\t\b\u0001.   \n[\u0002] J. Lee and A. J. Smith, \\Branch prediction strategies and branch target bu\u000ber design,\" IEEE Computer, pp. \u0006{\u0002\u0002, January \u0001\t\b\u0004.   \n[\u0003] T. Y. Yeh and Y. N. Patt, \\A comparison of dynamic branch predictors that use two levels of branch history,\" in Proceedings of the \u00020th Annual International Symposium on Computer Architecture, pp. \u0002\u0005\u0007{\u0002\u0006\u0006, May \u0001\t\t\u0003.   \n[\u0004] M. D. Smith, M. Johnson, and M. A. Horowitz, \\Limits on multiple instruction issue,\" in Proceedings of the \u0003rd International Conference on Architectural Support for Programming Languages and Operating Systems, pp. \u0002\t0{\u00030\u0002, April \u0001\t\b\t.   \n[\u0005] D. W. Wall, \\Limits of instruction-level parallelism,\" in Proceedings of the \u0004th International Conference on Architectural Support for Programming Languages and Operating Systems, pp. \u0001\u0007\u0006{\u0001\b\b, April \u0001\t\t\u0001.   \n[\u0006] M. Butler, T. Yeh, Y. Patt, M. Alsup, H. Scales, and M. Shebanow, \\Single instruction stream parallelism is greater than two,\" in Proceedings of the \u0001\bth International Symposium on Computer Architecture, pp. \u0002\u0007\u0006{\u0002\b\u0006, May \u0001\t\t\u0001. [\u0007] P. Y. Hsu and E. S. Davidson, \\Highly concurrent scalar processing,\" in Proceedings of the \u0001\u0003th International Symposium on Computer Architecture, pp. \u0003\b\u0006{\u0003\t\u0005, June \u0001\t\b\u0006. [\b] B. R. Rau, D. W. L. Yen, W. Yen, and R. A. Towle, \\The Cydra \u0005 departmental supercomputer,\" IEEE Computer, vol. \u0002\u0002, pp. \u0001\u0002{\u0003\u0005, January \u0001\t\b\t. [\t] J. R. Allen, K. Kennedy, C. Porter\feld, and J. Warren, \\Conversion of control dependence to data dependence,\" in Proceedings of the \u00010th ACM Symposium on Principles of Programming Languages, pp. \u0001\u0007\u0007{\u0001\b\t, January \u0001\t\b\u0003.   \n[\u00010] J. C. Park and M. S. Schlansker, \\On predicated execution,\" Tech. Rep. HPL-\t\u0001-\u0005\b, Hewlett Packard Laboratories, Palo Alto, CA, May \u0001\t\t\u0001.   \n[\u0001\u0001] S. A. Mahlke, D. C. Lin, W. Y. Chen, R. E. Hank, and R. A. Bringmann, \\E\u000bective compiler support for predicated execution using the hyperblock,\" in Proceedings of the \u0002\u0005th International Symposium on Microarchitecture, pp. \u0004\u0005{\u0005\u0004, December \u0001\t\t\u0002.   \n[\u0001\u0002] S. A. Mahlke, R. E. Hank, R. A. Bringmann, J. C. Gyllenhaal, D. M. Gallagher, and W. W. Hwu, \\Characterizing the impact of predicated execution on branch prediction,\" in Proceedings of the \u0002\u0007th International Symposium on Microarchitecture, pp. \u0002\u0001\u0007{\u0002\u0002\u0007, December \u0001\t\t\u0004.   \n[\u0001\u0003] G. R. Beck, D. W. Yen, and T. L. Anderson, \\The Cydra \u0005 minisupercomputer: Architecture and implementation,\" The Journal of Supercomputing, vol. \u0007, pp. \u0001\u0004\u0003{\u0001\b0, January \u0001\t\t\u0003.   \n[\u0001\u0004] D. N. Pnevmatikatos and G. S. Sohi, \\Guarded execution and branch prediction in dynamic ILP processors,\" in Proceedings of the \u0002\u0001st International Symposium on Computer Architecture, pp. \u0001\u00020{\u0001\u0002\t, April \u0001\t\t\u0004.   \n[\u0001\u0005] V. Kathail, M. S. Schlansker, and B. R. Rau, \\HPL playdoh architecture speci\fcation: Version \u0001.0,\" Tech. Rep. HPL\u0003-\b0, Hewlett-Packard Laboratories, Palo Alto, CA \t\u0004\u00030\u0003, February \u0001\t\t\u0004.   \n[\u0001\u0006] M. Schlansker, V. Kathail, and S. Anik, \\Height reduction of control recurrences for ILP processors,\" in Proceedings of the \u0002\u0007th International Symposium on Microarchitecture, pp. \u00040{ \u0005\u0001, December \u0001\t\t\u0004.   \n[\u0001\u0007] Hewlett-Packard Company, Cupertino, CA, PA-RISC \u0001.\u0001 Architecture and Instruction Set Reference Manual, \u0001\t\t0.   \n[\u0001\b] D. S. Blickstein et al., \\The GEM optimizing compiler system,\" Digital Technical Journal, vol. \u0004, pp. \u0001\u0002\u0001{\u0001\u0003\u0006, \u0001\t\t\u0002.   \n[\u0001\t] P. G. Lowney et al., \\The Multi\row trace scheduling compiler,\" The Journal of Supercomputing, vol. \u0007, pp. \u0005\u0001{\u0001\u0004\u0002, January \u0001\t\t\u0003.   \n[\u00020] W. W. Hwu et al., \\The Superblock: An e\u000bective technique for VLIW and superscalar compilation,\" The Journal of Supercomputing, vol. \u0007, pp. \u0002\u0002\t{\u0002\u0004\b, January \u0001\t\t\u0003.  "}